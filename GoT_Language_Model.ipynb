{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model for Game of Thrones\n",
    "\n",
    "*This notebook is part of the tutorial \"Modelling Sequences with Deep Learning\" presented at the ODSC London Conference in November 2019.*\n",
    "\n",
    "In this notebook, we will build a neural network model that can understand Game of Thrones language and concepts and even write its own passages. The architecture we will use is a **recurrent neural network (RNN)** with **LSTM cells** to boost the model's ability to remember longer-term information within the text. \n",
    "\n",
    "The framework we will use to build the models is `Keras`. Keras is a high-level neural networks API - it acts as a user-friendly layer on top of lower-level frameworks (Tensorflow, Theano, or CNTK), and allows you to build neural networks in an intuitive, layer-by-layer way. \n",
    "\n",
    "<img src=\"books.jpg\" alt=\"Picture of Game of Thrones books\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to language models\n",
    "\n",
    "Learning a **language model (LM)** is a classic modelling task in the field of **natural language processing (NLP)**. Since LMs learn to understand the structure and content of a text corpus, they are invaluable in applications where the quality or originality of text segments are being assessed. \n",
    "\n",
    "Language models are often used **generatively**, as in smartphone keyboard apps, to predict future text based on a **seed sequence**. \n",
    "\n",
    "For example, which word should follow the sequence \"the cat is on the\"? Good guesses are words like \"mat\", \"bed\", \"sofa\", and we would hope that our model would learn to assign high probabilities to these semantically relevant terms. We would hope that words like \"the\", \"hi\", and \"banana\" would be assigned low probabilities. \n",
    "\n",
    "## How are language models trained?\n",
    "\n",
    "All you need to train a language model is a text corpus - **no annotation or labelling of the data is required**. However, language modelling is treated as a **supervised classification task**. The idea is that we extract training data by **sliding a window over the corpus**, and generating input-output pairs that way. More exactly:\n",
    "\n",
    "![Building a dataset for training a language model](lm_data.png)\n",
    "\n",
    "So here, we are sliding a window of some size over the corpus in order to generate sequences of words (here, sequences of 2 words each). Then:\n",
    "+ The initial 2 words in each sequences is our **input** (or **features** or **X values**)\n",
    "+ The final word in each sequence is our **output** (or **label** or **y values**)\n",
    "\n",
    "The model is then trained to use the input words (the **context**) to predict the final word. \n",
    "\n",
    "## Considerations when building the dataset\n",
    "There are a few decisions you have to make with how you will build this dataset. For example:\n",
    "+ Are you going to treat the text as a sequence at the **word level** or the **character level**? \n",
    "\n",
    "    + **The arguments for using words are**: there is a lot of information in words since that's how we structure language. And the length of sequences the model has to deal with and remember will be much shorter, leading to greater coherence. \n",
    "    + **The arguments for using characters are**: the size of the input space is much more manageable (there are fewer characters than words), and you gain the ability to handle unknown words and generate new words.\n",
    "    + **You could also work at the sub-word level**: this is a bit of a happy medium - words are broken down into their components. \n",
    "    \n",
    "+ Are you going to **scrub the text squeaky clean** or do you want the model to learn to deal with **noise**, perhaps at a cost of a hit to performance?\n",
    "+ What sort of **window size** should you be using?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Building a Toy Language Model First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching straight into the Game of Thrones language modelling problem, let's work with a smaller first and understand all of the steps involved. This way, you can more easily understand and track how all of the input, intermediate steps, and output is behaving. \n",
    "\n",
    "Let's use the following poem from Lord of the Rings as our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus = ['All that is gold does not glitter',\n",
    "               'Not all those who wander are lost;',\n",
    "               'The old that is strong does not wither,',\n",
    "               'Deep roots are not reached by the frost.',\n",
    "               'From the ashes, a fire shall be woken,',\n",
    "               'A light from the shadows shall spring;',\n",
    "               'Renewed shall be blade that was broken,',\n",
    "               'The crownless again shall be king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, the first thing we need to do is **tokenisation** - break the text up into individual units or **tokens**. \n",
    "\n",
    "We can use the text tokeniser from the `Keras` library for this, and specify that we want to treat all text as lowercase, generate tokens by splitting on a space character, and view text at the word level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokeniser = Tokenizer(lower=True, split=' ', char_level=False)\n",
    "tiny_corpus = ' '.join(tiny_corpus)\n",
    "tokeniser.fit_on_texts([tiny_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokeniser identifies tokens in the corpus and assigns an index to each word in the vocabulary. We can check which index corresponds to which word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this tokeniser to convert (**encode**) our original corpus to a sequence of indices correponding to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_corpus = tokeniser.texts_to_sequences([tiny_corpus])[0]\n",
    "encoded_corpus[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always get back to the words by reversing this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser.sequences_to_texts([encoded_corpus[0:7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a dataset of sequences that we will use for training and evaluating our language model. \n",
    "\n",
    "Let's use a window size of 3 and slide this over the integer-encoded corpus to build our dataset: a **list of lists of length 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "window_size = 3\n",
    "for i in range(0, len(encoded_corpus)):\n",
    "    sequences.append(encoded_corpus[i:i+window_size])\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that at the end there we have sequences that are not length 3, since we run out of text. We can quickly **pad the sequences with zeroes** to keep the data size consistent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = np.max([len(sequence) for sequence in sequences])\n",
    "sequences = pad_sequences(sequences, \n",
    "                          maxlen=max_sequence_length, \n",
    "                          padding='pre')\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better. \n",
    "\n",
    "Finally, let's break the sequences down into our input data (X; our matrix of features) and our output data (y; our vector of labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x[0:2] for x in sequences])\n",
    "y = np.array([x[2] for x in sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example our input features for the first 5 data points are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And their corresponding labels are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing we need to do is reformat our label vector y into a **one-hot vector format**. The word index numbers are not actually meaningful (no ordinal relationship) but are discrete classes. We also want to calculate probabilities of word, where a probability of 1 for the correct word is the optimal prediction. \n",
    "\n",
    "We can convert the label vector y to a matrix of one-hot vectors using keras' `to_categorical` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "vocabulary_size = len(tokeniser.word_index)+1\n",
    "y = to_categorical(y, num_classes=vocabulary_size)\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, we have gone from a raw dataset of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To a formatted dataset ready to be input to a learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example features: ', *X[0:5], sep='\\n')\n",
    "print('Example labels: ', *y[0:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Setting up the language model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset sorted out, it's time to think about how we want to approach the modelling problem.\n",
    "\n",
    "Let's build this small recurrent neural network with LSTM units:\n",
    "\n",
    "![tiny_network](small_network.png)\n",
    "\n",
    "To explain this network:\n",
    "+ Our **input layer** represents input into the network. The size of the input layer is the size of the vocabulary of our corpus (+1).\n",
    "+ We then have an **embedding layer** immediately after the input layer, which will learn **word embeddings** for us (continuous representation of the discrete words in our vocabulary; see my explanatory blog post on embeddings [here](https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2). An embedding layer first changes your integer-encoded input to a one-hot vector format, followed by a fully-connected layer (with some regularisation and constraints), where the learned weight matrix functions as our word embeddings.\n",
    "+ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Keras` code, we would build this network like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=10, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(units=vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of this code block:\n",
    "+ Keras allows the sequential layer-by-layer building of neural network models using its `Sequential` API.\n",
    "+ The input layer is assumed, we don't need to explicitly build it.\n",
    "+ The first layer we add is our `Embedding` layer. The input dimensionality is our vocabulary size (the size of our input layer), and let's give this embedding layer a small size of 10 neurons. This means each word will get represented as a real-valued vector of length 10. We state the the length of inputs the network should expect is 2. \n",
    "+ Next, we add the workhorse of the network - our layer of `LSTM` neurons. Let's make the layer have 50 of these neurons (which is not a lot). We leave all other options to the default (activation functions, initialisation,etc.)\n",
    "+ Finally, as our output layer, we add a `Dense` fully-connected layer and softmax it. This means that the output of the network will be a vector of probabilities (summing to 1) spread across all the words of our vocabulary (see example below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine our model so far using Keras' `model.summary()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summarises the number of parameters in our model and where they are.\n",
    "+ 390 parameters from $39*10$\n",
    "+ 12200 LSTM parameters from $4*(10*50 + 50*50 + 50)$\n",
    "+ 1989 parameters from $50*39 + 39$\n",
    "\n",
    "Now that we have defined the network, we need to do a `model.compile()` to signify that we have finished building the network and want to define how training should proceed. Specifically, we need to provide:\n",
    "+ Which loss function we want to use (i.e. what is the goal the model is optimising for as it trains, or what signal is it following in order to improve)\n",
    "+ Which optimiser we want to use to do our gradient updates (Adam, Adagrad, RMSProp, Nesterov momentum, etc.)\n",
    "+ Any metrics we want to calculate and output during training in order to keep track of progress. Let's keep track of accuracy, which is just the percentage of predictions that the model gets right. \n",
    "\n",
    "We can just use sensible defaults for now. Since our task is a multiclass classification task, a sensible loss metric to use is **categorical cross-entropy**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll avoid dumping equations on you and just say that:\n",
    "+ The model's categorical cross-entropy loss will be **low** when the network generally predicts the next words correctly. This means it tends to assign higher probability to the correct word.\n",
    "+ A training loss of zero means that the network always assigns a probability of 1 to the correct word and 0 to all other words - its predictions are perfect (in the training set).\n",
    "+ The model's categorical cross-entropy loss will be **high** when the network generally doesn't predict the next words well. This means it tends to incorrectly assign high probabilities to incorrect words.\n",
    "\n",
    "During the training process, the model optimises its internal parameters such that training loss is minimised (for an explanation of how this happens, read about backpropagation and gradient descent [here]())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Training the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the network is compiled, we can begin training it for some time (for some number of **epochs** - which is the number of times the network sees your training data). \n",
    "\n",
    "Hopefully, as model training proceeds, we will see that the training loss steadily decreases and the accuracy increases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X, y, epochs=50, verbose=2)\n",
    "model.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the model trained! Training is very fast because our dataset is tiny and the network is small. The accuracy doesn't look that bad either (though of course the model is likely to be **overfitted**; see later section). \n",
    "\n",
    "There's a few different things you can do with a trained Keras sequence model. You can see all the options by typing `model.` followed by a `tab` in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Using the trained model to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the most interesting thing to do now is use the trained model to make new predictions. For this, we can use the `model.predict_classes()` method. \n",
    "\n",
    "We hope that the model will predict the next word given a seed sequence well, i.e. that it learned about word structure from our poem corpus. For instance, given the seed sequence \"shall be\", we hope the model predicts the correct, observed next words like \"king\", \"broken\", and \"blade\".\n",
    "\n",
    "However, we can't just run `model.predict_classes()` on raw text data like \"shall be\", since the text data has to first be tokenised, assigned to an integer index, and reshaped into the correct array dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_sequence = 'shall be'\n",
    "seed_sequence_encoded = tokeniser.texts_to_sequences([seed_sequence])[0]\n",
    "print('Encoded seed sequence: %s' % seed_sequence_encoded)\n",
    "seed_sequence_encoded = np.array(seed_sequence_encoded).reshape(-1,2)\n",
    "print('Formatted encoded seed sequence: %s' % seed_sequence_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the trained model to make a prediction for the next word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_index = model.predict_classes(seed_sequence_encoded)\n",
    "print('Prediction for the next word index: %s' % prediction_index)\n",
    "print('This index corresponds to word: %s' % tokeniser.sequences_to_texts([prediction_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that looks like a decent prediction for the next word!\n",
    "\n",
    "Rather than just have the 1 best prediction, it would be interesting to see the probabilities assigned to each possible next word. With a bit of manoeuvring we can get these scores out: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class_indices = list(range(0, vocabulary_size+1))\n",
    "\n",
    "df = pd.DataFrame(list(zip(class_indices, \n",
    "                      [tokeniser.sequences_to_texts([[index]])[0] for index in class_indices],\n",
    "                       model.predict(seed_sequence_encoded)[0],\n",
    "                       np.round(model.predict(seed_sequence_encoded)[0],5))),\n",
    "                  columns=['index', 'word', 'probability', 'rounded_probability'])\n",
    "\n",
    "df.sort_values('probability', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, it looks like the network does indeed assign the highest probabilities to the 3 words that actually occur in the corpus! It's fun to see that such a small network can produce sensible results on such a small dataset. \n",
    "\n",
    "Let's try another example with a different seed sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_sequence = 'does not'\n",
    "seed_sequence_encoded = tokeniser.texts_to_sequences([seed_sequence])[0]\n",
    "print('Encoded seed sequence: %s' % seed_sequence_encoded)\n",
    "seed_sequence_encoded = np.array(seed_sequence_encoded).reshape(-1,2)\n",
    "print('Formatted encoded seed sequence: %s' % seed_sequence_encoded)\n",
    "df = pd.DataFrame(list(zip(class_indices, \n",
    "                      [tokeniser.sequences_to_texts([[index]])[0] for index in class_indices],\n",
    "                       model.predict(seed_sequence_encoded)[0],\n",
    "                       np.round(model.predict(seed_sequence_encoded)[0],5))),\n",
    "                  columns=['index', 'word', 'probability', 'rounded_probability'])\n",
    "df.sort_values('probability', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that also looks correct.\n",
    "\n",
    "Rather than predicting just the next 1 word, would be nice to just let the network write continuous text for us, given some seed sequence starting point. Let's package up the above code into a function that lets us do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_sequence(seed_sequence,\n",
    "                        length_to_write,\n",
    "                        model, \n",
    "                        tokeniser, \n",
    "                        input_length,\n",
    "                        verbose=True):\n",
    "    \"\"\"\n",
    "    Generates text using a trained language\n",
    "    model and seed sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Using seed sequence: \"%s\"' % seed_sequence)\n",
    "    sequence = seed_sequence\n",
    "    \n",
    "    for i in range(length_to_write):\n",
    "        \n",
    "        # tokenise and encode the seed sequence\n",
    "        encoded_sequence = tokeniser.texts_to_sequences([sequence])[0]\n",
    "        assert len(encoded_sequence)>=input_length, \\\n",
    "            'ERROR: seed sequence must be at least %s words.' % input_length\n",
    "        encoded_sequence = encoded_sequence[-input_length:]\n",
    "        encoded_sequence = np.array(encoded_sequence).reshape(-1,input_length)\n",
    "\n",
    "        # predict the next word index and corresponding word\n",
    "        prediction_index = model.predict_classes(encoded_sequence)\n",
    "        prediction = tokeniser.sequences_to_texts([prediction_index])\n",
    "        \n",
    "        if verbose:\n",
    "            print('Sequence so far: %s' % sequence)\n",
    "            print('Seed sequence encoded: %s' % encoded_sequence)\n",
    "            print('Most likely next word is {0} (index {1})'.format(prediction, prediction_index[0]))\n",
    "\n",
    "        sequence += ' ' + prediction[0]\n",
    "    \n",
    "    print('Output:\\n' + sequence)\n",
    "    \n",
    "#     return sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_sequence(\"all that\", 5,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's write some more text, but let's turn off the verbosity of the function so we just get the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_sequence(\"the light\", 5,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's kind of artsy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, writing a longer passage this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_sequence(\"ashes are\", 10,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tiny model only knows the few words in the poem so this is a bit gibberish :) But it's still interesting to see.\n",
    "\n",
    "This is pretty much all there is to a basic language model. Now, let's tackle a real corpus (Game of Thrones) and build a bigger, more powerful model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Building a language model for Game of Thrones text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technical approach we'll take to building a GoT language model is pretty similar, with the major difference being the dataset. We are going to need access to a lot of GoT text - preferably, both the books and the subtitles from the HBO show. \n",
    "\n",
    "### i. Identifying some datasets\n",
    "\n",
    "Interestingly, there seems to already be a rich ecosystem of technical work surrounding GoT content. \n",
    "\n",
    "Check out projects like:\n",
    "+ The [Network of Thrones](https://networkofthrones.wordpress.com/) blog for network analyses of characters (e.g. which character is the most 'central' to the story?)\n",
    "+ An [API of Ice and Fire](https://anapioficeandfire.com) for grabbing various structured data about the universe\n",
    "+ And [this Reddit post](https://www.reddit.com/r/datasets/comments/769nhw/game_of_thrones_datasets_xpost_from_rfreefolk/) for a list of various datasets compiled about GoT.\n",
    "\n",
    "Maybe it's just me, but even despite these resources, I still couldn't actually find the raw text from the books and TV show. \n",
    "\n",
    "I did eventually come across 2 Kaggle datasets that contained exactly what I wanted:\n",
    "1. [Plain text files of all the books](https://www.kaggle.com/muhammedfathi/game-of-thrones-book-files/download) \n",
    "2. [Subtitle data for the episodes](https://filmora.wondershare.com/video-editing-tips/game-of-thrones-subtitles.html)\n",
    "    \n",
    "A bit of initial manual + regex clean up later, and you get the files included in this repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Grabbing all text data from the Game of Thrones books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've got a few books in our current directory in .txt format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these .txt files in the current directory:\n",
      "Book_1_A_Game_of_Thrones.txt\n",
      "Book_2_A_Clash_of_Kings.txt\n",
      "Book_3_A_Storm_of_Swords.txt\n",
      "Book_4_A_Feast_for_Crows.txt\n",
      "Book_5_A_Dance_with_Dragons.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "book_txt_files = sorted(glob.glob('*.txt'))\n",
    "print('Found these .txt files in the current directory:', *book_txt_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a function to extract all of the text in these files, glue it together, and flatten the resulting list of lists into a single mega GoT list of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iteration_utilities import flatten\n",
    "\n",
    "def grab_book_data(txt_files):\n",
    "    \"\"\"\n",
    "    Grabb text data from a set of text files.\n",
    "    \"\"\"\n",
    "\n",
    "    # keep all text segments in this list\n",
    "    all_text_segments = []   \n",
    "    \n",
    "    # iterate over each book file\n",
    "    for txt_file in txt_files:\n",
    "    \n",
    "        print('Extracting text from file \"%s\"...' % txt_file)\n",
    "        # open file\n",
    "        with open(txt_file, 'r') as file:\n",
    "            data = file.read()\n",
    "            print('Found {0} lines of text in this book.'.format(len(data.split('\\n'))))\n",
    "            print('First few lines:\\n %s\\n' % ' '.join(data.split('\\n')[0:5]))  \n",
    "            all_text_segments.append(data)\n",
    "            \n",
    "    return ''.join(list(flatten(all_text_segments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use it to put all the book text data in one place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file \"Book_1_A_Game_of_Thrones.txt\"...\n",
      "Found 14002 lines of text in this book.\n",
      "First few lines:\n",
      " A GAME OF THRONES  PROLOGUE  “We should start back,” Gared urged as the woods began to grow dark around them.\n",
      "\n",
      "Extracting text from file \"Book_2_A_Clash_of_Kings.txt\"...\n",
      "Found 15765 lines of text in this book.\n",
      "First few lines:\n",
      " A CLASH OF KINGS  PROLOGUE  The comet’s tail spread across the dawn, a red slash that bled above the crags of Dragonstone like a wound in the pink and purple sky.\n",
      "\n",
      "Extracting text from file \"Book_3_A_Storm_of_Swords.txt\"...\n",
      "Found 19641 lines of text in this book.\n",
      "First few lines:\n",
      " A STORM OF SWORDS  PROLOGUE  The day was grey and bitter cold, and the dogs would not take the scent.\n",
      "\n",
      "Extracting text from file \"Book_4_A_Feast_for_Crows.txt\"...\n",
      "Found 16225 lines of text in this book.\n",
      "First few lines:\n",
      " A FEAST FOR CROWS  PROLOGUE  Dragons,” said Mollander. He snatched a withered apple off the ground and tossed it hand to hand.\n",
      "\n",
      "Extracting text from file \"Book_5_A_Dance_with_Dragons.txt\"...\n",
      "Found 18889 lines of text in this book.\n",
      "First few lines:\n",
      " A DANCE WITH DRAGONS  PROLOGUE  The night was rank with the smell of man.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_data = grab_book_data(book_txt_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly summarise the amount of data we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in this corpus: 84518\n",
      "The number of words in this corpus: 1724951\n"
     ]
    }
   ],
   "source": [
    "# count lines and words\n",
    "print('The number of lines in this corpus: {0}\\n'\n",
    "      'The number of words in this corpus: {1}'.format(len(book_data.split('\\n')),\n",
    "                                                       len(book_data.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Grabbing all text data from the Game of Thrones show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subtitle data is a bit more complicated to grab because it's in JSON file format, and also frankly the text is a bit messy - there's markup tags, music note symbols, and various other odd non-textual things. \n",
    "\n",
    "We have the following `.json` subtitle files in our current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these .json files in the current directory:\n",
      "Season_1_Subtitles.json\n",
      "Season_2_Subtitles.json\n",
      "Season_3_Subtitles.json\n",
      "Season_4_Subtitles.json\n",
      "Season_5_Subtitles.json\n",
      "Season_6_Subtitles.json\n",
      "Season_7_Subtitles.json\n"
     ]
    }
   ],
   "source": [
    "subtitle_json_files = sorted(glob.glob(\"*.json\"))\n",
    "print('Found these .json files in the current directory:', *subtitle_json_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to write a function to get the data out. The function below will:\n",
    "+ **Iterate** over a given list of json subtitle files, **open** each file and **parse** the json\n",
    "+ **Sort** the subtitles by index. At the moment, the indices are sorted as strings (so, e.g. '1' is followed by '11') so we need to convert the indices to integers and sort them numerically. This is important to get right because otherwise the subtitles are jumbled out of order! \n",
    "+ And finally we **extract** the subtitle text and **append** to a master list (which we reformat by flattening) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def grab_subtitle_data(subtitle_json_files, verbose=True):\n",
    "    \"\"\"\n",
    "    Grabbing GoT subtitle data from json files.\n",
    "    \"\"\"\n",
    "\n",
    "    # keep all text segments in this list\n",
    "    all_text_segments = []\n",
    "\n",
    "    # iterate over each subtitles file\n",
    "    for season, subtitles_file in enumerate(subtitle_json_files):\n",
    "\n",
    "        # open subtitle file\n",
    "        with open(subtitles_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # iterate over episodes in the season\n",
    "        for episode in data.keys():\n",
    "            episode_data = {int(key):value for key,value in data[episode].items()}\n",
    "            episode_data = sorted(episode_data.items()) # deal with sorting by line (as integer) s\n",
    "            episode_text_segments = list(dict(episode_data).values())\n",
    "            print('Found {0} text segments in Season {1} '\n",
    "                  'Episode \"{2}\".'.format(len(episode_text_segments), \n",
    "                                          season, \n",
    "                                          episode.split('.')[0]))\n",
    "            if verbose:\n",
    "                print('First few segments:\\n%s' % '\\n'.join(episode_text_segments[0:5]))            \n",
    "            all_text_segments.append(episode_text_segments)\n",
    "            \n",
    "    return list(flatten(all_text_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 559 text segments in Season 0 Episode \"Game Of Thrones S01E01 Winter Is Coming\".\n",
      "Found 571 text segments in Season 0 Episode \"Game Of Thrones S01E02 The Kingsroad\".\n",
      "Found 740 text segments in Season 0 Episode \"Game Of Thrones S01E03 Lord Snow\".\n",
      "Found 754 text segments in Season 0 Episode \"Game Of Thrones S01E04 Cripples, Bastards, And Broken Things\".\n",
      "Found 741 text segments in Season 0 Episode \"Game Of Thrones S01E05 The Wolf And The Lion\".\n",
      "Found 583 text segments in Season 0 Episode \"Game Of Thrones S01E06 A Golden Crown\".\n",
      "Found 775 text segments in Season 0 Episode \"Game Of Thrones S01E07 You Win Or You Die\".\n",
      "Found 666 text segments in Season 0 Episode \"Game Of Thrones S01E08 The Pointy End\".\n",
      "Found 679 text segments in Season 0 Episode \"Game Of Thrones S01E09 Baelor\".\n",
      "Found 590 text segments in Season 0 Episode \"Game Of Thrones S01E10 Fire And Blood\".\n",
      "Found 700 text segments in Season 1 Episode \"Game Of Thrones S02E01 The North Remembers\".\n",
      "Found 755 text segments in Season 1 Episode \"Game Of Thrones S02E02 The Night Lands\".\n",
      "Found 654 text segments in Season 1 Episode \"Game Of Thrones S02E03 What Is Dead May Never Die\".\n",
      "Found 619 text segments in Season 1 Episode \"Game Of Thrones S02E04 Garden Of Bones\".\n",
      "Found 781 text segments in Season 1 Episode \"Game Of Thrones S02E05 The Ghost Of Harrenhal\".\n",
      "Found 730 text segments in Season 1 Episode \"Game Of Thrones S02E06 The Old Gods And The New\".\n",
      "Found 762 text segments in Season 1 Episode \"Game Of Thrones S02E07 A Man Without Honor\".\n",
      "Found 775 text segments in Season 1 Episode \"Game Of Thrones S02E08 The Prince Of Winterfell\".\n",
      "Found 640 text segments in Season 1 Episode \"Game Of Thrones S02E09 Blackwater\".\n",
      "Found 641 text segments in Season 1 Episode \"Game Of Thrones S02E10 Valar Morghulis\".\n",
      "Found 637 text segments in Season 2 Episode \"Game Of Thrones S03E01 Valar Dohaeris\".\n",
      "Found 778 text segments in Season 2 Episode \"Game Of Thrones S03E02 Dark Wings, Dark Words\".\n",
      "Found 661 text segments in Season 2 Episode \"Game Of Thrones S03E03 Walk Of Punishment\".\n",
      "Found 703 text segments in Season 2 Episode \"Game Of Thrones S03E04 And Now His Watch Is Ended\".\n",
      "Found 821 text segments in Season 2 Episode \"Game Of Thrones S03E05 Kissed By Fire\".\n",
      "Found 652 text segments in Season 2 Episode \"Game Of Thrones S03E06 The Climb\".\n",
      "Found 714 text segments in Season 2 Episode \"Game Of Thrones S03E07 The Bear And The Maiden Fair\".\n",
      "Found 576 text segments in Season 2 Episode \"Game Of Thrones S03E08 Second Sons\".\n",
      "Found 524 text segments in Season 2 Episode \"Game Of Thrones S03E09 The Rains Of Castamere\".\n",
      "Found 785 text segments in Season 2 Episode \"Game Of Thrones S03E10 Mhysa\".\n",
      "Found 756 text segments in Season 3 Episode \"Game Of Thrones S04E01 Two Swords\".\n",
      "Found 636 text segments in Season 3 Episode \"Game Of Thrones S04E02 The Lion And The Rose\".\n",
      "Found 753 text segments in Season 3 Episode \"Game Of Thrones S04E03 Breaker Of Chains\".\n",
      "Found 634 text segments in Season 3 Episode \"Game Of Thrones S04E04 Oathkeeper\".\n",
      "Found 664 text segments in Season 3 Episode \"Game Of Thrones S04E05 First Of His Name\".\n",
      "Found 622 text segments in Season 3 Episode \"Game Of Thrones S04E06 The Laws Of Gods And Men\".\n",
      "Found 695 text segments in Season 3 Episode \"Game Of Thrones S04E07 Mockingbird\".\n",
      "Found 662 text segments in Season 3 Episode \"Game Of Thrones S04E08 The Mountain And The Viper\".\n",
      "Found 451 text segments in Season 3 Episode \"Game Of Thrones S04E09 The Watchers On The Wall\".\n",
      "Found 613 text segments in Season 3 Episode \"Game Of Thrones S04E10 The Children\".\n",
      "Found 0 text segments in Season 3 Episode \"season4\".\n",
      "Found 631 text segments in Season 4 Episode \"Game Of Thrones S05E01 The Wars To Come\".\n",
      "Found 748 text segments in Season 4 Episode \"Game Of Thrones S05E02 The House Of Black And White\".\n",
      "Found 782 text segments in Season 4 Episode \"Game Of Thrones S05E03 High Sparrow\".\n",
      "Found 601 text segments in Season 4 Episode \"Game Of Thrones S05E04 Sons Of The Harpy\".\n",
      "Found 647 text segments in Season 4 Episode \"Game Of Thrones S05E05 Kill The Boy\".\n",
      "Found 628 text segments in Season 4 Episode \"Game Of Thrones S05E06 Unbowed, Unbent, Unbroken\".\n",
      "Found 700 text segments in Season 4 Episode \"Game Of Thrones S05E07 The Gift\".\n",
      "Found 658 text segments in Season 4 Episode \"Game Of Thrones S05E08 Hardhome\".\n",
      "Found 481 text segments in Season 4 Episode \"Game Of Thrones S05E09 The Dance Of Dragons\".\n",
      "Found 563 text segments in Season 4 Episode \"Game Of Thrones S05E10 Mother's Mercy\".\n",
      "Found 385 text segments in Season 5 Episode \"Game Of Thrones S06E01 The Red Woman\".\n",
      "Found 480 text segments in Season 5 Episode \"Game Of Thrones S06E02 Home\".\n",
      "Found 598 text segments in Season 5 Episode \"Game Of Thrones S06E03 Oathbreaker\".\n",
      "Found 640 text segments in Season 5 Episode \"Game Of Thrones S06E04 Book of the Stranger\".\n",
      "Found 644 text segments in Season 5 Episode \"Game Of Thrones S06E05 The Door\".\n",
      "Found 605 text segments in Season 5 Episode \"Game Of Thrones S06E06 Blood of My Blood\".\n",
      "Found 615 text segments in Season 5 Episode \"Game Of Thrones S06E07 The Broken Man\".\n",
      "Found 661 text segments in Season 5 Episode \"Game Of Thrones S06E08 No One\".\n",
      "Found 446 text segments in Season 5 Episode \"Game Of Thrones S06E09 Battle of the Bastards\".\n",
      "Found 605 text segments in Season 5 Episode \"Game Of Thrones S06E10 The Winds of Winter\".\n",
      "Found 753 text segments in Season 6 Episode \"Game Of Thrones S07E01 Dragonstone\".\n",
      "Found 812 text segments in Season 6 Episode \"Game Of Thrones S07E02 Stormborn\".\n",
      "Found 983 text segments in Season 6 Episode \"Game Of Thrones S07E03 The Queen's Justice\".\n",
      "Found 608 text segments in Season 6 Episode \"Game Of Thrones S07E04 The Spoils Of War\".\n",
      "Found 764 text segments in Season 6 Episode \"Game Of Thrones S07E05 Eastwatch\".\n",
      "Found 796 text segments in Season 6 Episode \"Game Of Thrones S07E06 Beyond The Wall\".\n",
      "Found 958 text segments in Season 6 Episode \"Game Of Thrones S07E07 The Dragon And The Wolf\".\n"
     ]
    }
   ],
   "source": [
    "subtitle_data = grab_subtitle_data(subtitle_json_files, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final array of subtitle data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Easy, boy.',\n",
       " \"What do you expect? They're savages.\",\n",
       " 'One lot steals a goat from another lot,',\n",
       " \"before you know it they're ripping each other to pieces.\",\n",
       " \"I've never seen wildlings do a thing like this.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitle_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can summarise the dataset size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of text segments in this corpus: 44844\n",
      "The number of words in this corpus: 244447\n"
     ]
    }
   ],
   "source": [
    "# count lines and words\n",
    "all_subtitle_text = '\\n'.join(subtitle_data)\n",
    "print('The number of text segments in this corpus: {0}\\n'\n",
    "      'The number of words in this corpus: {1}'.format(len(all_subtitle_text.split('\\n')),\n",
    "                                                       len(all_subtitle_text.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Combining the book and subtitle datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put the book and subtitle data together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_data = book_data+all_subtitle_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(book_data)\n",
    "del(all_subtitle_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And report on the size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in the final corpus: 129361\n",
      "The number of words in the final corpus: 1969397\n"
     ]
    }
   ],
   "source": [
    "print('The number of lines in the final corpus: {0}\\n'\n",
    "      'The number of words in the final corpus: {1}'.format(len(got_data.split('\\n')),\n",
    "                                                            len(got_data.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 2 million words to play with, which should help our language model tremendously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A GAME OF THRONES\\n\\nPROLOGUE\\n\\n“We should start back,” Gared urged as the woods began to grow dark aro'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' And night is falling.”\\n\\nSer Waymar Royce glanced at the sky with disinterest. “It does that every d'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "got_data[1000:1100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process to make the sequence datasets is the same as before. The only difference is that we'll use longer sequences as our input (`window_size` is now 6), so we're taking into account more text before making our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for this corpus is: 30350\n"
     ]
    }
   ],
   "source": [
    "# tokenise the data\n",
    "tokeniser = Tokenizer(lower=True, split=' ', char_level=False)\n",
    "tokeniser.fit_on_texts([got_data])\n",
    "vocabulary_size = len(tokeniser.word_index)+1\n",
    "print('The vocabulary size for this corpus is: %s' % vocabulary_size)\n",
    "\n",
    "# encode the corpus using the fitted tokeniser\n",
    "encoded_corpus = tokeniser.texts_to_sequences([got_data])[0]\n",
    "\n",
    "# generate sequences\n",
    "sequences = []\n",
    "window_size = 6\n",
    "for i in range(0, len(encoded_corpus)):\n",
    "    sequences.append(encoded_corpus[i:i+window_size])\n",
    "\n",
    "# pad the sequences at the end so each sequence is the same length\n",
    "max_sequence_length = np.max([len(sequence) for sequence in sequences])\n",
    "sequences = pad_sequences(sequences, \n",
    "                          maxlen=max_sequence_length, \n",
    "                          padding='pre')\n",
    "\n",
    "# separate sequences into input arrays X \n",
    "# and the output label vector y\n",
    "X = np.array([seq[0:window_size-1] for seq in sequences])\n",
    "y = np.array([seq[window_size-1] for seq in sequences])\n",
    "y = to_categorical(y, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2094848, 30350)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('GoT_X_features.npz', X)\n",
    "# np.save('GoT_y_labels.npz', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, our features look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    5,   972,     6,  3796, 12141],\n",
       "       [  972,     6,  3796, 12141,   322],\n",
       "       [    6,  3796, 12141,   322,   122],\n",
       "       [ 3796, 12141,   322,   122,  1131],\n",
       "       [12141,   322,   122,  1131,    62]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our labels look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful extra step: we should **split the dataset into a train and test set**. The main reason for this is that it will help us get a better estimate of the model's true \"in the wild\" performance, since we can evaluate its performance on data that *wasn't* used in training. \n",
    "\n",
    "Evaluating a model on data that was used for training is cheating, since it's already seen that data before, and hence will do unrealistically well when making predictions on it because it has **overfit**.\n",
    "\n",
    "We will also shuffle the entries, since otherwise our dataset first contains Book 1, then Book 2, ..., Book 5 then finally the subtitle data, whereas we want the model to learn from each source simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_X = X[0:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_y = y[0:500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # for tutorial: take a sample of the data for speed\n",
    "# # sample_size = 1000000\n",
    "# sampled_indices = np.random.choice(np.arange(len(y)), sample_size, replace=False)\n",
    "# small_X = X[sampled_indices]\n",
    "# small_y = y[sampled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(small_X, small_y, test_size=0.1, shuffle=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Setting up the language model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's build a slightly larger network:\n",
    "\n",
    "![Larger RNN language model](big_network.png)\n",
    "\n",
    "The main differences here are:\n",
    "+ Our word embeddings are bigger (100 rather than 50 dimensions), which should allow for richer representations of word meaning\n",
    "+ We have 2 LSTM layers instead of 1. This should allow the model to learn more complex, hierarchical representations of the text.\n",
    "+ We have added a dense (fully-connected) layer after the LSTM layers for some additional processing capacity (perhaps, again, allowing for higher-level conceptual representations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Keras` code, we would build the network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, 50, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very similar code to before, but we have reason to think that this network will be much more complex and nuanced than the previous one:\n",
    "+ The dataset we are using is much larger and richer than the toy dataset\n",
    "+ The network we are training is larger and deeper, and should have more expressive power\n",
    "\n",
    "We can summarise the **model structure and parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 50)             1517500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30350)             3065350   \n",
      "=================================================================\n",
      "Total params: 4,733,750\n",
      "Trainable params: 4,733,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compile the finished model and specify some **training settings**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vii. Training the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can start the training run by passing the training data to the model. This would take a reasonably long time to train - it would be helpful to have access to a **GPU** to run this on (e.g. via Google Colab, AWS/GCP, your own GPU) to make use of computation **parallelisation** and drastically reduce training time.\n",
    "\n",
    "Since it's a longer training run, we would also ideally want to save some intermediate results while training is happening. One way to do this is using Keras' `ModelCheckpoint` utility. To save some disc space, you can specify that you only want to save a new checkpoint file when something about the model has improved (commonly, validation accuracy or validation loss). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 405000 samples, validate on 45000 samples\n",
      "Epoch 1/50\n",
      "405000/405000 [==============================] - 853s 2ms/step - loss: 6.4852 - accuracy: 0.0820 - val_loss: 6.1031 - val_accuracy: 0.1097\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.10967, saving model to GoT_Language_Model_01_0.11.hdf5\n",
      "Epoch 2/50\n",
      "405000/405000 [==============================] - 830s 2ms/step - loss: 5.8193 - accuracy: 0.1209 - val_loss: 5.8581 - val_accuracy: 0.1232\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.10967 to 0.12318, saving model to GoT_Language_Model_02_0.12.hdf5\n",
      "Epoch 3/50\n",
      "405000/405000 [==============================] - 842s 2ms/step - loss: 5.5300 - accuracy: 0.1339 - val_loss: 5.7700 - val_accuracy: 0.1316\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.12318 to 0.13156, saving model to GoT_Language_Model_03_0.13.hdf5\n",
      "Epoch 4/50\n",
      "405000/405000 [==============================] - 854s 2ms/step - loss: 5.3574 - accuracy: 0.1431 - val_loss: 5.7377 - val_accuracy: 0.1361\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.13156 to 0.13609, saving model to GoT_Language_Model_04_0.14.hdf5\n",
      "Epoch 5/50\n",
      "405000/405000 [==============================] - 871s 2ms/step - loss: 5.2377 - accuracy: 0.1498 - val_loss: 5.7233 - val_accuracy: 0.1388\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.13609 to 0.13882, saving model to GoT_Language_Model_05_0.14.hdf5\n",
      "Epoch 6/50\n",
      "405000/405000 [==============================] - 880s 2ms/step - loss: 5.1439 - accuracy: 0.1549 - val_loss: 5.7415 - val_accuracy: 0.1412\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.13882 to 0.14118, saving model to GoT_Language_Model_06_0.14.hdf5\n",
      "Epoch 7/50\n",
      "405000/405000 [==============================] - 882s 2ms/step - loss: 5.0647 - accuracy: 0.1599 - val_loss: 5.7582 - val_accuracy: 0.1427\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.14118 to 0.14271, saving model to GoT_Language_Model_07_0.14.hdf5\n",
      "Epoch 8/50\n",
      "405000/405000 [==============================] - 882s 2ms/step - loss: 4.9921 - accuracy: 0.1644 - val_loss: 5.7995 - val_accuracy: 0.1432\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.14271 to 0.14320, saving model to GoT_Language_Model_08_0.14.hdf5\n",
      "Epoch 9/50\n",
      "405000/405000 [==============================] - 894s 2ms/step - loss: 4.9255 - accuracy: 0.1689 - val_loss: 5.8570 - val_accuracy: 0.1436\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.14320 to 0.14358, saving model to GoT_Language_Model_09_0.14.hdf5\n",
      "Epoch 10/50\n",
      "405000/405000 [==============================] - 900s 2ms/step - loss: 4.8641 - accuracy: 0.1731 - val_loss: 5.8590 - val_accuracy: 0.1431\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.14358\n",
      "Epoch 11/50\n",
      "405000/405000 [==============================] - 906s 2ms/step - loss: 4.8057 - accuracy: 0.1773 - val_loss: 5.9504 - val_accuracy: 0.1418\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.14358\n",
      "Epoch 12/50\n",
      "405000/405000 [==============================] - 900s 2ms/step - loss: 4.7493 - accuracy: 0.1814 - val_loss: 5.9983 - val_accuracy: 0.1434\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.14358\n",
      "Epoch 13/50\n",
      "405000/405000 [==============================] - 908s 2ms/step - loss: 4.6948 - accuracy: 0.1855 - val_loss: 6.1115 - val_accuracy: 0.1429\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.14358\n",
      "Epoch 14/50\n",
      "405000/405000 [==============================] - 907s 2ms/step - loss: 4.6446 - accuracy: 0.1891 - val_loss: 6.1423 - val_accuracy: 0.1408\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.14358\n",
      "Epoch 15/50\n",
      "405000/405000 [==============================] - 896s 2ms/step - loss: 4.5981 - accuracy: 0.1921 - val_loss: 6.2144 - val_accuracy: 0.1413\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.14358\n",
      "Epoch 16/50\n",
      "405000/405000 [==============================] - 926s 2ms/step - loss: 4.5539 - accuracy: 0.1962 - val_loss: 6.2844 - val_accuracy: 0.1406\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.14358\n",
      "Epoch 17/50\n",
      "405000/405000 [==============================] - 926s 2ms/step - loss: 4.5113 - accuracy: 0.1994 - val_loss: 6.3697 - val_accuracy: 0.1393\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.14358\n",
      "Epoch 18/50\n",
      "405000/405000 [==============================] - 920s 2ms/step - loss: 4.4716 - accuracy: 0.2020 - val_loss: 6.4329 - val_accuracy: 0.1380\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.14358\n",
      "Epoch 19/50\n",
      "405000/405000 [==============================] - 924s 2ms/step - loss: 4.4351 - accuracy: 0.2052 - val_loss: 6.5201 - val_accuracy: 0.1375\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.14358\n",
      "Epoch 20/50\n",
      "405000/405000 [==============================] - 934s 2ms/step - loss: 4.3975 - accuracy: 0.2085 - val_loss: 6.6131 - val_accuracy: 0.1358\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.14358\n",
      "Epoch 21/50\n",
      "405000/405000 [==============================] - 950s 2ms/step - loss: 4.3637 - accuracy: 0.2107 - val_loss: 6.6500 - val_accuracy: 0.1351\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.14358\n",
      "Epoch 22/50\n",
      "405000/405000 [==============================] - 948s 2ms/step - loss: 4.3299 - accuracy: 0.2134 - val_loss: 6.6909 - val_accuracy: 0.1341\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.14358\n",
      "Epoch 23/50\n",
      "405000/405000 [==============================] - 943s 2ms/step - loss: 4.2963 - accuracy: 0.2163 - val_loss: 6.7899 - val_accuracy: 0.1325\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.14358\n",
      "Epoch 24/50\n",
      "405000/405000 [==============================] - 950s 2ms/step - loss: 4.2666 - accuracy: 0.2194 - val_loss: 6.8360 - val_accuracy: 0.1339\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.14358\n",
      "Epoch 25/50\n",
      "405000/405000 [==============================] - 953s 2ms/step - loss: 4.2366 - accuracy: 0.2208 - val_loss: 6.9495 - val_accuracy: 0.1316\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.14358\n",
      "Epoch 26/50\n",
      "405000/405000 [==============================] - 925s 2ms/step - loss: 4.2063 - accuracy: 0.2235 - val_loss: 7.0318 - val_accuracy: 0.1294\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.14358\n",
      "Epoch 27/50\n",
      "405000/405000 [==============================] - 911s 2ms/step - loss: 4.1799 - accuracy: 0.2260 - val_loss: 7.1395 - val_accuracy: 0.1294\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.14358\n",
      "Epoch 28/50\n",
      "405000/405000 [==============================] - 918s 2ms/step - loss: 4.1548 - accuracy: 0.2282 - val_loss: 7.1695 - val_accuracy: 0.1300\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.14358\n",
      "Epoch 29/50\n",
      "405000/405000 [==============================] - 911s 2ms/step - loss: 4.1283 - accuracy: 0.2303 - val_loss: 7.3634 - val_accuracy: 0.1299\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.14358\n",
      "Epoch 30/50\n",
      "405000/405000 [==============================] - 928s 2ms/step - loss: 4.1061 - accuracy: 0.2326 - val_loss: 7.2732 - val_accuracy: 0.1277\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.14358\n",
      "Epoch 31/50\n",
      "405000/405000 [==============================] - 923s 2ms/step - loss: 4.0840 - accuracy: 0.2346 - val_loss: 7.3815 - val_accuracy: 0.1276\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.14358\n",
      "Epoch 32/50\n",
      "405000/405000 [==============================] - 929s 2ms/step - loss: 4.0607 - accuracy: 0.2368 - val_loss: 7.3874 - val_accuracy: 0.1263\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.14358\n",
      "Epoch 33/50\n",
      "405000/405000 [==============================] - 913s 2ms/step - loss: 4.0450 - accuracy: 0.2380 - val_loss: 7.3907 - val_accuracy: 0.1260\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.14358\n",
      "Epoch 34/50\n",
      "405000/405000 [==============================] - 918s 2ms/step - loss: 4.0248 - accuracy: 0.2403 - val_loss: 7.6188 - val_accuracy: 0.1253\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.14358\n",
      "Epoch 35/50\n",
      "405000/405000 [==============================] - 922s 2ms/step - loss: 4.0058 - accuracy: 0.2420 - val_loss: 7.7819 - val_accuracy: 0.1258\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.14358\n",
      "Epoch 36/50\n",
      "405000/405000 [==============================] - 921s 2ms/step - loss: 3.9921 - accuracy: 0.2433 - val_loss: 7.7447 - val_accuracy: 0.1240\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.14358\n",
      "Epoch 37/50\n",
      "405000/405000 [==============================] - 923s 2ms/step - loss: 3.9750 - accuracy: 0.2448 - val_loss: 7.7381 - val_accuracy: 0.1227\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.14358\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405000/405000 [==============================] - 930s 2ms/step - loss: 3.9600 - accuracy: 0.2463 - val_loss: 7.7869 - val_accuracy: 0.1228\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.14358\n",
      "Epoch 39/50\n",
      "405000/405000 [==============================] - 954s 2ms/step - loss: 3.9466 - accuracy: 0.2477 - val_loss: 7.7522 - val_accuracy: 0.1196\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.14358\n",
      "Epoch 40/50\n",
      "405000/405000 [==============================] - 950s 2ms/step - loss: 3.9316 - accuracy: 0.2492 - val_loss: 7.9866 - val_accuracy: 0.1228\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.14358\n",
      "Epoch 41/50\n",
      "405000/405000 [==============================] - 951s 2ms/step - loss: 3.9189 - accuracy: 0.2504 - val_loss: 7.9392 - val_accuracy: 0.1209\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.14358\n",
      "Epoch 42/50\n",
      "405000/405000 [==============================] - 958s 2ms/step - loss: 3.9074 - accuracy: 0.2513 - val_loss: 7.9770 - val_accuracy: 0.1176\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.14358\n",
      "Epoch 43/50\n",
      "405000/405000 [==============================] - 940s 2ms/step - loss: 3.8966 - accuracy: 0.2523 - val_loss: 8.0705 - val_accuracy: 0.1189\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.14358\n",
      "Epoch 44/50\n",
      "405000/405000 [==============================] - 938s 2ms/step - loss: 3.8872 - accuracy: 0.2535 - val_loss: 7.9736 - val_accuracy: 0.1166\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.14358\n",
      "Epoch 45/50\n",
      "405000/405000 [==============================] - 938s 2ms/step - loss: 3.8737 - accuracy: 0.2558 - val_loss: 8.0603 - val_accuracy: 0.1160\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.14358\n",
      "Epoch 46/50\n",
      "405000/405000 [==============================] - 949s 2ms/step - loss: 3.8617 - accuracy: 0.2566 - val_loss: 8.1554 - val_accuracy: 0.1152\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.14358\n",
      "Epoch 47/50\n",
      "405000/405000 [==============================] - 938s 2ms/step - loss: 3.8544 - accuracy: 0.2571 - val_loss: 8.2324 - val_accuracy: 0.1165\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.14358\n",
      "Epoch 48/50\n",
      "405000/405000 [==============================] - 928s 2ms/step - loss: 3.8461 - accuracy: 0.2583 - val_loss: 8.2726 - val_accuracy: 0.1182\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.14358\n",
      "Epoch 49/50\n",
      "405000/405000 [==============================] - 917s 2ms/step - loss: 3.8331 - accuracy: 0.2595 - val_loss: 8.2354 - val_accuracy: 0.1130\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.14358\n",
      "Epoch 50/50\n",
      "405000/405000 [==============================] - 905s 2ms/step - loss: 3.8253 - accuracy: 0.2601 - val_loss: 8.3627 - val_accuracy: 0.1154\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.14358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14cbf9790>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNCOMMENT AND RUN THIS CELL TO TRAIN THE MODEL YOURSELF\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_filename=\"GoT_Language_Model_{epoch:02d}_{val_accuracy:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_filename, \n",
    "                             monitor='val_accuracy', \n",
    "                             save_best_only=True, \n",
    "                             mode='max',  # 'best' file maximises validation_accuracy \n",
    "                             verbose=1, )\n",
    "model.fit(X_train, y_train, epochs=50, validation_split=0.1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_trained_GoT_language_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, to save time, I will just **load a model** that I already trained. \n",
    "\n",
    "For reference, this model was really accessible to train - it was trained overnight on my MacBook, so there's no special GPU supercomputer involved. The model was still improving quite rapidly at that point, so we would see even better performance if the model were given enough time to reach **convergence** (\"finish\" learning, or at least hit serious diminishing returns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('final_trained_GoT_language_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viii. Exploring our Game of Thrones language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarise the model's performance on the test set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall test accuracy: 0.11624\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_predictions = loaded_model.predict_classes(X_test)\n",
    "print('Overall test accuracy: {0}'.format(accuracy_score(np.argmax(y_test, axis=1), test_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems a bit low, but language is complicated and flexible. What does this performance mean in practical terms? We can examine some of the correct answers vs. predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seed Sequence</th>\n",
       "      <th>Actual Next Word</th>\n",
       "      <th>Predicted Next Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>free folk here craster serves</td>\n",
       "      <td>no</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>that place since the day</td>\n",
       "      <td>her</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>grow around the gravel swallowing</td>\n",
       "      <td>it</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>theon had given the matter</td>\n",
       "      <td>no</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>but oversweet to his taste</td>\n",
       "      <td>“if</td>\n",
       "      <td>jewels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>so sore he could scarcely</td>\n",
       "      <td>walk</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>wore on their wedding night</td>\n",
       "      <td>tyrion</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>and split them laying the</td>\n",
       "      <td>logs</td>\n",
       "      <td>stew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>i love him ” sansa</td>\n",
       "      <td>wailed</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>wheeled his horse about and</td>\n",
       "      <td>trotted</td>\n",
       "      <td>galloped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>in strong arms and his</td>\n",
       "      <td>hand</td>\n",
       "      <td>brothers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>him he had been the</td>\n",
       "      <td>most</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>a bird after lord eddard</td>\n",
       "      <td>with</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>sons had watched as their</td>\n",
       "      <td>father’s</td>\n",
       "      <td>brother’s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>stomped through the trees and</td>\n",
       "      <td>vanished</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>it looks uncle i thought</td>\n",
       "      <td>the</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>rather liked robert baratheon great</td>\n",
       "      <td>blustering</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>stern “the freys are your</td>\n",
       "      <td>lady</td>\n",
       "      <td>enemies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>galley white hart plots to</td>\n",
       "      <td>slip</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>sweet child ” she said</td>\n",
       "      <td>“i</td>\n",
       "      <td>“i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>who rule there the world</td>\n",
       "      <td>will</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>the litter was grey his</td>\n",
       "      <td>eyes</td>\n",
       "      <td>face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>earnestly “the blood … there’s</td>\n",
       "      <td>bloodstains</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>” he laughed he had</td>\n",
       "      <td>a</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>“call yourself king of the</td>\n",
       "      <td>iron</td>\n",
       "      <td>andals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>himself caught like a rat</td>\n",
       "      <td>in</td>\n",
       "      <td>joust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>wolverine calm as still water</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>face the mouth especially no</td>\n",
       "      <td>simple</td>\n",
       "      <td>priest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>he swore “what lord eddard</td>\n",
       "      <td>means</td>\n",
       "      <td>stark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>more interested in the pair</td>\n",
       "      <td>that</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>i cut you ” “i’ll</td>\n",
       "      <td>promise</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>” she bent over the</td>\n",
       "      <td>tub</td>\n",
       "      <td>table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>floor the going was faster</td>\n",
       "      <td>and</td>\n",
       "      <td>than</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>succession even though i’m older</td>\n",
       "      <td>”</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>wrinkled faces of the aged</td>\n",
       "      <td>i</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>tree the old gods know</td>\n",
       "      <td>when</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>a pen they were the</td>\n",
       "      <td>only</td>\n",
       "      <td>only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>not told you the worst</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>hesitated he had no inkling</td>\n",
       "      <td>what</td>\n",
       "      <td>sworn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>a stone ladder than proper</td>\n",
       "      <td>steps</td>\n",
       "      <td>fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>you could see it from</td>\n",
       "      <td>miles</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>will pay him any heed</td>\n",
       "      <td>i</td>\n",
       "      <td>“all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>want to ” “all men</td>\n",
       "      <td>must</td>\n",
       "      <td>duel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>than that “i know he</td>\n",
       "      <td>must</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>high seat as lannister’s men</td>\n",
       "      <td>reached</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>around her slender hips “how</td>\n",
       "      <td>dare</td>\n",
       "      <td>many</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>scanning the ridgetops warily through</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>ned demanded his voice ringing</td>\n",
       "      <td>“why</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>advice the bedding was stuffed</td>\n",
       "      <td>with</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>she descended quickly jumping down</td>\n",
       "      <td>the</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Seed Sequence Actual Next Word Predicted Next Word\n",
       "0           free folk here craster serves               no                 the\n",
       "1                that place since the day              her                  of\n",
       "2       grow around the gravel swallowing               it                 the\n",
       "3              theon had given the matter               no                  he\n",
       "4              but oversweet to his taste              “if              jewels\n",
       "5               so sore he could scarcely             walk               allow\n",
       "6             wore on their wedding night           tyrion                   ”\n",
       "7               and split them laying the             logs                stew\n",
       "8                      i love him ” sansa           wailed                said\n",
       "9             wheeled his horse about and          trotted            galloped\n",
       "10                 in strong arms and his             hand            brothers\n",
       "11                    him he had been the             most               first\n",
       "12               a bird after lord eddard             with                 had\n",
       "13              sons had watched as their         father’s           brother’s\n",
       "14          stomped through the trees and         vanished                  he\n",
       "15               it looks uncle i thought              the                  he\n",
       "16    rather liked robert baratheon great       blustering                 and\n",
       "17              stern “the freys are your             lady             enemies\n",
       "18             galley white hart plots to             slip                 lie\n",
       "19                 sweet child ” she said               “i                  “i\n",
       "20               who rule there the world             will                 was\n",
       "21                the litter was grey his             eyes                face\n",
       "22         earnestly “the blood … there’s      bloodstains                   a\n",
       "23                    ” he laughed he had                a                been\n",
       "24             “call yourself king of the             iron              andals\n",
       "25              himself caught like a rat               in               joust\n",
       "26          wolverine calm as still water              the                 the\n",
       "27           face the mouth especially no           simple              priest\n",
       "28             he swore “what lord eddard            means               stark\n",
       "29            more interested in the pair             that                  of\n",
       "30                      i cut you ” “i’ll          promise                  be\n",
       "31                    ” she bent over the              tub               table\n",
       "32             floor the going was faster              and                than\n",
       "33       succession even though i’m older                ”                 and\n",
       "34             wrinkled faces of the aged                i               green\n",
       "35                 tree the old gods know             when                   ”\n",
       "36                    a pen they were the             only                only\n",
       "37                 not told you the worst               of                  of\n",
       "38            hesitated he had no inkling             what               sworn\n",
       "39             a stone ladder than proper            steps             fearful\n",
       "40                  you could see it from            miles                 the\n",
       "41                  will pay him any heed                i                “all\n",
       "42                     want to ” “all men             must                duel\n",
       "43                   than that “i know he             must                  is\n",
       "44           high seat as lannister’s men          reached                 was\n",
       "45           around her slender hips “how             dare                many\n",
       "46  scanning the ridgetops warily through              the                 the\n",
       "47         ned demanded his voice ringing             “why                  in\n",
       "48         advice the bedding was stuffed             with                  in\n",
       "49     she descended quickly jumping down              the                  to"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seed_sequences = tokeniser.sequences_to_texts(X_test[0:50])\n",
    "actual_next_words = tokeniser.sequences_to_texts([np.argmax(y_test, axis=1)[0:50]])[0].split(' ')\n",
    "prediction_index = model.predict_classes(X_test[0:50])\n",
    "prediction_vector = tokeniser.sequences_to_texts([prediction_index])\n",
    "predictions = prediction_vector[0].split(' ')\n",
    "df = pd.DataFrame(list(zip(test_seed_sequences,\n",
    "                           actual_next_words,\n",
    "                           predictions)),\n",
    "                  columns=['Seed Sequence', 'Actual Next Word', 'Predicted Next Word'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like even where the model doesn't get the prediction correct, its prediction does at least seem plausible. \n",
    "\n",
    "That said, there is tons that can be done to improve this model (see final section in this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ix. Gather Round for a New Tale..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have the language model write some Game of Thrones text for us (since GRR Martin certainly isn't going to!). \n",
    "\n",
    "We can use the same function as before to continuously feed in a seed sequence to the model, generate one word, and then append the generated word to the seed sequence. In this way, the model uses its own previous output as input to itself in the future. \n",
    "\n",
    "Let's write some text (sort of cherry picked lengths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"I would not have expected\"\n",
      "Output:\n",
      "I would not have expected no fear ” “help me a king dark as fresh shadows better catelyn thought herself nipple off the rear from the window “my lord ” the greatjon tossed a sullen wooden arm osmynd had said “what would we do again is this a pack ” shae nipped\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"I would not have expected\", 47,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"Once upon a time the\"\n",
      "Output:\n",
      "Once upon a time the next morning when the blades poured out in the courtyard and the distant halls of woth had never been a bitter man struggling on the wall and drank the whole terms of the false feast cat ” he said sharply “we have a torch ” the merchant bear said “i have no choice\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"Once upon a time the\", 53,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"The start of the story\"\n",
      "Output:\n",
      "The start of the story and the rest of the traitor stannis ” he said “i will not see ” he said “i have no choice for that joffrey’s name day he had been born in the crypts ” the knight said “are you a splendid bastard ”\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"The start of the story\", 43,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all looks like a decent start - I especially like the merchant bear, and am shocked by the curveball that Joffrey was born in the crypts of Winterfell. Other segments sound like weird GoT beat poetry, and I can almost hear the soft accompanying bongo beats.\n",
    "\n",
    "It's clear from these samples that the model has clearly learned something about both language structure and GoT content, but admittedly it's still a bit clunky. Check out the section at the bottom for suggestions on how to improve the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x. Generating more creative output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the model generatively and write longer stories, you'll see that it can sometimes get stuck in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"The men with the swords\"\n",
      "Output:\n",
      "The men with the swords beating in the solar of his youth “oh yes ” the king said “i have a role in yours ” the raven agreed “nor island remained in the whispering wood and the others had been a pig sworn years the women had been allowed to hear the banners in the dust of the burning fork of the trident he’ll be able to catch the horse with a hand of his own nurse he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had been a fortnight past he had\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"The men with the swords\", 200,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because our function `write_text_sequences` will always greedily choose the most probable next word as the next token as it generates text. This is the cause of these repetitive loops. \n",
    "\n",
    "Ideally, we want to give the model a bit more space to be creative than this. The easiest way to do this is instead of using the **most probable next word** as our prediction, we can **sample from all possible words proportionally to their probability**. This will help introduce some fun linguistic variety into our generated text. \n",
    "\n",
    "The most probable words are still, of course, most likely to be chosen, but there is now space for less probable words to be used as well. We are trading off (potentially rigid) local correctness for (potentially noisy) creativity. \n",
    "\n",
    "We can modify our `write_text_sequences` to have an option to use this probabilistic sampling approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_sequence(seed_sequence,\n",
    "                        length_to_write,\n",
    "                        model, \n",
    "                        tokeniser, \n",
    "                        input_length,\n",
    "                        verbose=True,\n",
    "                        use_sampling=True):\n",
    "    \"\"\"\n",
    "    Generates text using a trained language\n",
    "    model and seed sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Using seed sequence: \"%s\"' % seed_sequence)\n",
    "    sequence = seed_sequence\n",
    "    \n",
    "    for i in range(length_to_write):\n",
    "        \n",
    "        # tokenise and encode the seed sequence\n",
    "        encoded_sequence = tokeniser.texts_to_sequences([sequence])[0]\n",
    "        assert len(encoded_sequence)>=input_length, \\\n",
    "            'ERROR: seed sequence must be at least %s words.' % input_length\n",
    "        encoded_sequence = encoded_sequence[-input_length:]\n",
    "        encoded_sequence = np.array(encoded_sequence).reshape(-1,input_length)\n",
    "\n",
    "        # predict the next word index and corresponding word\n",
    "        if use_sampling:\n",
    "            next_word_probabilities = model.predict_proba(encoded_sequence)[0]\n",
    "            next_word_indices = range(0, vocabulary_size)\n",
    "            prediction_index = np.random.choice(next_word_indices, size=1, p=next_word_probabilities)             \n",
    "        else: \n",
    "            prediction_index = model.predict_classes(encoded_sequence)\n",
    "        \n",
    "        # convert prediction index to actual word\n",
    "        prediction = tokeniser.sequences_to_texts([prediction_index])\n",
    "        \n",
    "        if verbose:\n",
    "            print('Sequence so far: %s' % sequence)\n",
    "            print('Seed sequence encoded: %s' % encoded_sequence)\n",
    "            print('Most likely next word is {0} (index {1})'.format(prediction, prediction_index[0]))\n",
    "\n",
    "        # append prediction to the sequence\n",
    "        sequence += ' ' + prediction[0]\n",
    "    \n",
    "    print('Output:\\n' + sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this addition helps us get out of the infinite loop of fortnight past:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"The men with the swords\"\n",
      "Output:\n",
      "The men with the swords circled to say how badly his fathers were shagga knights mail and golden crabs ” mirri maz duur wept as lord walder rushed to sound behind him she heard coming upon six nights beheaded the last son who had refuge the city blow the hand stood outside the frozen wooden grassy ground with the golden wool in his place broke from these party are nine the two crows but the shouting seemed to lose with once he knew anything but nothing and they she had known afterward she had had beside the whole king as truth let me make out walls on the ridge dany could have done he had told him his death at lord of yours stark how know he read cradled the girl rising but deep strangely slowly “open them you be he’s alone as a poor master for sudden mouth who remained of the eastern road sound uncomfortable like blood the quiet with his hand “it won’t do time he’ll see to me now pray when i felt ” “the man says if his way were back in front of him the young king always seasoned ” the voice offered thinking flapping stone eyes draped by\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"The men with the swords\", 200,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False, \n",
    "                    use_sampling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well.. we're definitely not trapped anymore! But now the text sounds absolutely bonkers. Let's see if we can put some breaks on this thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xi. Generating more creative (but controlled) output \n",
    "\n",
    "The main way of controlling how creative or random these sampling-based predictions are is by using a hyperparameter called `temperature`. Essentially:\n",
    "+ **Higher temperature** will emphasise the least likely predictions in a distribution - less likely predictions will have their probabilities increased. To remember this, think of \"hot\" = more randomness, just like with higher physical temperature leading to more random molecular motion. \n",
    "+ **Lower temperature** will downplay the less likely predictions. At the lowest temperatures, we are only ever considering the most likely prediction (our sampling starts to function like an `argmax` and we go back to the greedy approach)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a function to do this scaling of probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temp_to_softmax_probs(probs, temp, verbose=False):\n",
    "    \"\"\"\n",
    "    Rescales softmax probabilities using some given temperature.\n",
    "    \"\"\"\n",
    "    \n",
    "    # add a very small number to probabilities\n",
    "    # to avoid taking log of zero later (undefined) \n",
    "    epsilon = 10e-16 \n",
    "    probs = probs + epsilon\n",
    "\n",
    "    # take logs of probabilities\n",
    "    log_probs = np.log(probs)\n",
    "    \n",
    "    # the crucial step - divide the log probabilities by temperature\n",
    "    scaled_log_probs = log_probs / temp \n",
    "\n",
    "    # undo logging to get back to probabilities\n",
    "    new_probs = np.exp(scaled_log_probs) \n",
    "\n",
    "    # and renormalise so that probabilities sum to 1\n",
    "    normalised_probs = new_probs / np.sum(new_probs)\n",
    "    \n",
    "    if verbose:\n",
    "        print('1. Original probabilities:\\n%s\\n' % probs)\n",
    "        print('2. Log of probabilities:\\n%s\\n' % log_probs)\n",
    "        print('3. Temperature scaled log of probabilities:\\n%s\\n' % scaled_log_probs)\n",
    "        print('4. Back to pseudo-probabilities by undoing logging:\\n%s\\n' % new_probs)\n",
    "        print('5. Final normalised probabilities:\\n%s\\n' % normalised_probs)\n",
    "\n",
    "    return normalised_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out how temperature scaling of probability arrays happens by testing out this function in verbose mode. Let's scale the array `np.array([0.8, 0.1, 0.05, 0.05])` using different temperatures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### temperature=1 (should do nothing at all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original probabilities:\n",
      "[0.8  0.1  0.05 0.05]\n",
      "\n",
      "2. Log of probabilities:\n",
      "[-0.22314355 -2.30258509 -2.99573227 -2.99573227]\n",
      "\n",
      "3. Temperature scaled log of probabilities:\n",
      "[-0.22314355 -2.30258509 -2.99573227 -2.99573227]\n",
      "\n",
      "4. Back to pseudo-probabilities by undoing logging:\n",
      "[0.8  0.1  0.05 0.05]\n",
      "\n",
      "5. Final normalised probabilities:\n",
      "[0.8  0.1  0.05 0.05]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = apply_temp_to_softmax_probs(np.array([0.8, 0.1, 0.05, 0.05]), \n",
    "                            temp=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it's good to know that using a temperature of 1 does nothing at all to the probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### temperature=10 (should boost low probabilities and introduce more randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original probabilities:\n",
      "[0.8  0.1  0.05 0.05]\n",
      "\n",
      "2. Log of probabilities:\n",
      "[-0.22314355 -2.30258509 -2.99573227 -2.99573227]\n",
      "\n",
      "3. Temperature scaled log of probabilities:\n",
      "[-0.02231436 -0.23025851 -0.29957323 -0.29957323]\n",
      "\n",
      "4. Back to pseudo-probabilities by undoing logging:\n",
      "[0.97793277 0.79432823 0.74113445 0.74113445]\n",
      "\n",
      "5. Final normalised probabilities:\n",
      "[0.30048357 0.2440685  0.22772396 0.22772396]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = apply_temp_to_softmax_probs(np.array([0.8, 0.1, 0.05, 0.05]), \n",
    "                                temp=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high temperature of 10 really amplifies those low probabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### temperature=0.1 (should dampen out lower probabilities and boost already high probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Original probabilities:\n",
      "[0.8  0.1  0.05 0.05]\n",
      "\n",
      "2. Log of probabilities:\n",
      "[-0.22314355 -2.30258509 -2.99573227 -2.99573227]\n",
      "\n",
      "3. Temperature scaled log of probabilities:\n",
      "[ -2.23143551 -23.02585093 -29.95732274 -29.95732274]\n",
      "\n",
      "4. Back to pseudo-probabilities by undoing logging:\n",
      "[1.07374182e-01 1.00000000e-10 9.76562500e-14 9.76562500e-14]\n",
      "\n",
      "5. Final normalised probabilities:\n",
      "[9.99999999e-01 9.31322574e-10 9.09494701e-13 9.09494701e-13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = apply_temp_to_softmax_probs(np.array([0.8, 0.1, 0.05, 0.05]), \n",
    "                                temp=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a low temperature of 0.1 really freezes down those low probabilities, they are practically 0. \n",
    "\n",
    "How come the maths works? Essentially:\n",
    "+ Probabilities that are already big don't have big (negative) logarithms, so scaling them by multiplying/dividing by temperature won't make that much of a difference.\n",
    "+ But small probabilities have very big (negative) logarithms, so scaling them by multiplying/dividing by temperature can hugely change their values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add 1 line to our `write_text_sequence` function to make use of temperature (line 29):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_sequence(seed_sequence,\n",
    "                        length_to_write,\n",
    "                        model, \n",
    "                        tokeniser, \n",
    "                        input_length,\n",
    "                        verbose=True,\n",
    "                        use_sampling=True, \n",
    "                        temperature=1):\n",
    "    \"\"\"\n",
    "    Generates text using a trained language\n",
    "    model and seed sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Using seed sequence: \"%s\"' % seed_sequence)\n",
    "    sequence = seed_sequence\n",
    "    \n",
    "    for i in range(length_to_write):\n",
    "        \n",
    "        # tokenise and encode the seed sequence\n",
    "        encoded_sequence = tokeniser.texts_to_sequences([sequence])[0]\n",
    "        assert len(encoded_sequence)>=input_length, \\\n",
    "            'ERROR: seed sequence must be at least %s words.' % input_length\n",
    "        encoded_sequence = encoded_sequence[-input_length:]\n",
    "        encoded_sequence = np.array(encoded_sequence).reshape(-1,input_length)\n",
    "\n",
    "        # predict the next word index and corresponding word\n",
    "        if use_sampling:\n",
    "            next_word_probabilities = model.predict_proba(encoded_sequence)[0]\n",
    "            next_word_probabilities = apply_temp_to_softmax_probs(next_word_probabilities, temperature)\n",
    "            next_word_indices = range(0, vocabulary_size)\n",
    "            prediction_index = np.random.choice(next_word_indices, size=1, p=next_word_probabilities)             \n",
    "        else: \n",
    "            prediction_index = model.predict_classes(encoded_sequence)\n",
    "        \n",
    "        # convert prediction index to actual word\n",
    "        prediction = tokeniser.sequences_to_texts([prediction_index])\n",
    "        \n",
    "        if verbose:\n",
    "            print('Sequence so far: %s' % sequence)\n",
    "            print('Seed sequence encoded: %s' % encoded_sequence)\n",
    "            print('Most likely next word is {0} (index {1})'.format(prediction, prediction_index[0]))\n",
    "\n",
    "        # append prediction to the sequence\n",
    "        sequence += ' ' + prediction[0]\n",
    "    \n",
    "    print('Output:\\n' + sequence)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can control the creativity level of the text generation by changing the value of one argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"Jon Snow is the son\"\n",
      "Output:\n",
      "Jon Snow is the son of mine ” she said “i don’t think i shall find it too to see\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"Jon Snow is the son\", 15,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False, \n",
    "                    use_sampling=True, \n",
    "                    temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"Jon Snow is the son\"\n",
      "Output:\n",
      "Jon Snow is the son to joffrey’s way lift it in the wood and remark licked his hands “to tell\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"Jon Snow is the son\", 15,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False, \n",
    "                    use_sampling=True, \n",
    "                    temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mental text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"Jon Snow is the son\"\n",
      "Output:\n",
      "Jon Snow is the son for long barracks in all haste fall lines likewise found “ahhhh dothraki this first boy\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"Jon Snow is the son\", 15,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False, \n",
    "                    use_sampling=True, \n",
    "                    temperature=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this project gave you a taste of language models, that's all for this tutorial for now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Suggested Extensions\n",
    "\n",
    "Here are some suggestions for extending this work in order to build a more serious Game of Thrones language model:\n",
    "\n",
    "1. **Data**: Spend more time cleaning up the text corpus, there is definitely some weird stuff in there (e.g. I saw markup tags in the subtitle data)\n",
    "2. **Data**: Perhaps think about grabbing more data, maybe by scraping some of the fan Wikis.\n",
    "3. **Representation**: Use **pre-trained word embeddings** (e.g. FastText, GloVe, Word2Vec) and possibly update them during training\n",
    "4. **Representation**: Think about using **sub-word tokenisation** rather than word-based tokenisation\n",
    "8. **Modelling**: **Train for longer**, until convergence :) Monitor for overfitting using a validation set to early stop. \n",
    "5. **Modelling**: Look into using **regularisation techniques** (dropout, weight penalties) to improve model performance and generalisability\n",
    "6. **Modelling**: Experiment with different numbers of layers, sizes, activation functions, initialisation approaches, etc.\n",
    "7. **Modelling**: Optimise some of the **hyperparameters** in the model (learning rate, momentum, batch sizes)\n",
    "9. **Modelling**: Forget RNNs for language modelling completely and jump on the **Transformer hype train** ([choo](https://paperswithcode.com/task/language-modelling) [choo!](https://arxiv.org/abs/1904.09408)). \n",
    "10. **Modelling**: Try downloading a **pre-trained language model** (like **Google AI's BERT** or **OpenAI's GPT models** or **Carnegie Mellon/Google Brain's XLNet**) and fine-tuning it to Game of Thrones text. This is likely to give the easiest, biggest gains, since these models are pre-trained on massive corpora with a huge amount of GPUs. \n",
    "10. **Visualisation**: Try using **Tensorboard** to visualise the progression of model training and diagnose any weird behaviour. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
