{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model for Game of Thrones\n",
    "\n",
    "*This notebook is part of the tutorial \"Modelling Sequences with Deep Learning\" presented at the ODSC London Conference in November 2019.*\n",
    "\n",
    "In this notebook, we will build a neural network model that can understand Game of Thrones language and concepts and even write its own passages. The architecture we will use is a **recurrent neural network (RNN)** with **LSTM cells** to boost the model's ability to remember longer-term information within the text. \n",
    "\n",
    "The framework we will use to build the models is `Keras`. Keras is a high-level neural networks API - it acts as a user-friendly layer on top of lower-level frameworks (Tensorflow, Theano, or CNTK), and allows you to build neural networks in an intuitive, layer-by-layer way. \n",
    "\n",
    "<img src=\"books.jpg\" alt=\"Picture of Game of Thrones books\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to language models\n",
    "\n",
    "Learning a **language model (LM)** is a classic modelling task in the field of **natural language processing (NLP)**. Since LMs learn to understand the structure and content of a text corpus, they are invaluable in applications where the quality or originality of text segments are being assessed. \n",
    "\n",
    "Language models are often used **generatively**, as in smartphone keyboard apps, to predict future text based on a **seed sequence**. \n",
    "\n",
    "For example, which word should follow the sequence \"the cat is on the\"? Good guesses are words like \"mat\", \"bed\", \"sofa\", and we would hope that our model would learn to assign high probabilities to these semantically relevant terms. We would hope that words like \"the\", \"hi\", and \"banana\" would be assigned low probabilities. \n",
    "\n",
    "## How are language models trained?\n",
    "\n",
    "All you need to train a language model is a text corpus - **no annotation or labelling of the data is required**. However, language modelling is treated as a **supervised classification task**. The idea is that we extract training data by **sliding a window over the corpus**, and generating input-output pairs that way. More exactly:\n",
    "\n",
    "![Building a dataset for training a language model](lm_data.png)\n",
    "\n",
    "So here, we are sliding a window of some size over the corpus in order to generate sequences of words (here, sequences of 2 words each). Then:\n",
    "+ The initial 2 words in each sequences is our **input** (or **features** or **X values**)\n",
    "+ The final word in each sequence is our **output** (or **label** or **y values**)\n",
    "\n",
    "The model is then trained to use the input words (the **context**) to predict the final word. \n",
    "\n",
    "## Considerations when building the dataset\n",
    "There are a few decisions you have to make with how you will build this dataset. For example:\n",
    "+ Are you going to treat the text as a sequence at the **word level** or the **character level**? \n",
    "\n",
    "    + **The arguments for using words are**: there is a lot of information in words since that's how we structure language. And the length of sequences the model has to deal with and remember will be much shorter, leading to greater coherence. \n",
    "    + **The arguments for using characters are**: the size of the input space is much more manageable (there are fewer characters than words), and you gain the ability to handle unknown words and generate new words.\n",
    "    + **You could also work at the sub-word level**: this is a bit of a happy medium - words are broken down into their components. \n",
    "    \n",
    "+ Are you going to **scrub the text squeaky clean** or do you want the model to learn to deal with **noise**, perhaps at a cost of a hit to performance?\n",
    "+ What sort of **window size** should you be using?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Building a Toy Language Model First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching straight into the Game of Thrones language modelling problem, let's work with a smaller first and understand all of the steps involved. This way, you can more easily understand and track how all of the input, intermediate steps, and output is behaving. \n",
    "\n",
    "Let's use the following poem from Lord of the Rings as our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus = ['All that is gold does not glitter',\n",
    "               'Not all those who wander are lost;',\n",
    "               'The old that is strong does not wither,',\n",
    "               'Deep roots are not reached by the frost.',\n",
    "               'From the ashes, a fire shall be woken,',\n",
    "               'A light from the shadows shall spring;',\n",
    "               'Renewed shall be blade that was broken,',\n",
    "               'The crownless again shall be king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, the first thing we need to do is **tokenisation** - break the text up into individual units or **tokens**. \n",
    "\n",
    "We can use the text tokeniser from the `Keras` library for this, and specify that we want to treat all text as lowercase, generate tokens by splitting on a space character, and view text at the word level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokeniser = Tokenizer(lower=True, split=' ', char_level=False)\n",
    "tiny_corpus = ' '.join(tiny_corpus)\n",
    "tokeniser.fit_on_texts([tiny_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokeniser identifies tokens in the corpus and assigns an index to each word in the vocabulary. We can check which index corresponds to which word like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'not': 2,\n",
       " 'shall': 3,\n",
       " 'that': 4,\n",
       " 'be': 5,\n",
       " 'all': 6,\n",
       " 'is': 7,\n",
       " 'does': 8,\n",
       " 'are': 9,\n",
       " 'from': 10,\n",
       " 'a': 11,\n",
       " 'gold': 12,\n",
       " 'glitter': 13,\n",
       " 'those': 14,\n",
       " 'who': 15,\n",
       " 'wander': 16,\n",
       " 'lost': 17,\n",
       " 'old': 18,\n",
       " 'strong': 19,\n",
       " 'wither': 20,\n",
       " 'deep': 21,\n",
       " 'roots': 22,\n",
       " 'reached': 23,\n",
       " 'by': 24,\n",
       " 'frost': 25,\n",
       " 'ashes': 26,\n",
       " 'fire': 27,\n",
       " 'woken': 28,\n",
       " 'light': 29,\n",
       " 'shadows': 30,\n",
       " 'spring': 31,\n",
       " 'renewed': 32,\n",
       " 'blade': 33,\n",
       " 'was': 34,\n",
       " 'broken': 35,\n",
       " 'crownless': 36,\n",
       " 'again': 37,\n",
       " 'king': 38}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this tokeniser to convert (**encode**) our original corpus to a sequence of indices correponding to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 4, 7, 12, 8, 2, 13]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_corpus = tokeniser.texts_to_sequences([tiny_corpus])[0]\n",
    "encoded_corpus[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always get back to the words by reversing this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all that is gold does not glitter']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.sequences_to_texts([encoded_corpus[0:7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a dataset of sequences that we will use for training and evaluating our language model. \n",
    "\n",
    "Let's use a window size of 3 and slide this over the integer-encoded corpus to build our dataset: a **list of lists of length 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 4, 7],\n",
       " [4, 7, 12],\n",
       " [7, 12, 8],\n",
       " [12, 8, 2],\n",
       " [8, 2, 13],\n",
       " [2, 13, 2],\n",
       " [13, 2, 6],\n",
       " [2, 6, 14],\n",
       " [6, 14, 15],\n",
       " [14, 15, 16],\n",
       " [15, 16, 9],\n",
       " [16, 9, 17],\n",
       " [9, 17, 1],\n",
       " [17, 1, 18],\n",
       " [1, 18, 4],\n",
       " [18, 4, 7],\n",
       " [4, 7, 19],\n",
       " [7, 19, 8],\n",
       " [19, 8, 2],\n",
       " [8, 2, 20],\n",
       " [2, 20, 21],\n",
       " [20, 21, 22],\n",
       " [21, 22, 9],\n",
       " [22, 9, 2],\n",
       " [9, 2, 23],\n",
       " [2, 23, 24],\n",
       " [23, 24, 1],\n",
       " [24, 1, 25],\n",
       " [1, 25, 10],\n",
       " [25, 10, 1],\n",
       " [10, 1, 26],\n",
       " [1, 26, 11],\n",
       " [26, 11, 27],\n",
       " [11, 27, 3],\n",
       " [27, 3, 5],\n",
       " [3, 5, 28],\n",
       " [5, 28, 11],\n",
       " [28, 11, 29],\n",
       " [11, 29, 10],\n",
       " [29, 10, 1],\n",
       " [10, 1, 30],\n",
       " [1, 30, 3],\n",
       " [30, 3, 31],\n",
       " [3, 31, 32],\n",
       " [31, 32, 3],\n",
       " [32, 3, 5],\n",
       " [3, 5, 33],\n",
       " [5, 33, 4],\n",
       " [33, 4, 34],\n",
       " [4, 34, 35],\n",
       " [34, 35, 1],\n",
       " [35, 1, 36],\n",
       " [1, 36, 37],\n",
       " [36, 37, 3],\n",
       " [37, 3, 5],\n",
       " [3, 5, 38],\n",
       " [5, 38],\n",
       " [38]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "window_size = 3\n",
    "for i in range(0, len(encoded_corpus)):\n",
    "    sequences.append(encoded_corpus[i:i+window_size])\n",
    "\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that at the end there we have sequences that are not length 3, since we run out of text. We can quickly **pad the sequences with zeroes** to keep the data size consistent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  4,  7],\n",
       "       [ 4,  7, 12],\n",
       "       [ 7, 12,  8],\n",
       "       [12,  8,  2],\n",
       "       [ 8,  2, 13],\n",
       "       [ 2, 13,  2],\n",
       "       [13,  2,  6],\n",
       "       [ 2,  6, 14],\n",
       "       [ 6, 14, 15],\n",
       "       [14, 15, 16],\n",
       "       [15, 16,  9],\n",
       "       [16,  9, 17],\n",
       "       [ 9, 17,  1],\n",
       "       [17,  1, 18],\n",
       "       [ 1, 18,  4],\n",
       "       [18,  4,  7],\n",
       "       [ 4,  7, 19],\n",
       "       [ 7, 19,  8],\n",
       "       [19,  8,  2],\n",
       "       [ 8,  2, 20],\n",
       "       [ 2, 20, 21],\n",
       "       [20, 21, 22],\n",
       "       [21, 22,  9],\n",
       "       [22,  9,  2],\n",
       "       [ 9,  2, 23],\n",
       "       [ 2, 23, 24],\n",
       "       [23, 24,  1],\n",
       "       [24,  1, 25],\n",
       "       [ 1, 25, 10],\n",
       "       [25, 10,  1],\n",
       "       [10,  1, 26],\n",
       "       [ 1, 26, 11],\n",
       "       [26, 11, 27],\n",
       "       [11, 27,  3],\n",
       "       [27,  3,  5],\n",
       "       [ 3,  5, 28],\n",
       "       [ 5, 28, 11],\n",
       "       [28, 11, 29],\n",
       "       [11, 29, 10],\n",
       "       [29, 10,  1],\n",
       "       [10,  1, 30],\n",
       "       [ 1, 30,  3],\n",
       "       [30,  3, 31],\n",
       "       [ 3, 31, 32],\n",
       "       [31, 32,  3],\n",
       "       [32,  3,  5],\n",
       "       [ 3,  5, 33],\n",
       "       [ 5, 33,  4],\n",
       "       [33,  4, 34],\n",
       "       [ 4, 34, 35],\n",
       "       [34, 35,  1],\n",
       "       [35,  1, 36],\n",
       "       [ 1, 36, 37],\n",
       "       [36, 37,  3],\n",
       "       [37,  3,  5],\n",
       "       [ 3,  5, 38],\n",
       "       [ 0,  5, 38],\n",
       "       [ 0,  0, 38]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = np.max([len(sequence) for sequence in sequences])\n",
    "sequences = pad_sequences(sequences, \n",
    "                          maxlen=max_sequence_length, \n",
    "                          padding='pre')\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better. \n",
    "\n",
    "Finally, let's break the sequences down into our input data (X; our matrix of features) and our output data (y; our vector of labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x[0:2] for x in sequences])\n",
    "y = np.array([x[2] for x in sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example our input features for the first 5 data points are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  4],\n",
       "       [ 4,  7],\n",
       "       [ 7, 12],\n",
       "       [12,  8],\n",
       "       [ 8,  2]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And their corresponding labels are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 12,  8,  2, 13], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final thing we need to do is reformat our label vector y into a **one-hot vector format**. The word index numbers are not actually meaningful (no ordinal relationship) but are discrete classes. We also want to calculate probabilities of word, where a probability of 1 for the correct word is the optimal prediction. \n",
    "\n",
    "We can convert the label vector y to a matrix of one-hot vectors using keras' `to_categorical` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "vocabulary_size = len(tokeniser.word_index)+1\n",
    "y = to_categorical(y, num_classes=vocabulary_size)\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, we have gone from a raw dataset of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All that is gold does not glitter Not all those who wander are lost; The old that is strong does not wither, Deep roots are not reached by the frost. From the ashes, a fire shall be woken, A light from the shadows shall spring; Renewed shall be blade that was broken, The crownless again shall be king'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To a formatted dataset ready to be input to a learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example features: \n",
      "[6 4]\n",
      "[4 7]\n",
      "[ 7 12]\n",
      "[12  8]\n",
      "[8 2]\n",
      "Example labels: \n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Example features: ', *X[0:5], sep='\\n')\n",
    "print('Example labels: ', *y[0:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Setting up the language model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset sorted out, it's time to think about how we want to approach the modelling problem.\n",
    "\n",
    "Let's build this small recurrent neural network with LSTM units:\n",
    "\n",
    "![tiny_network](small.png)\n",
    "\n",
    "To explain this network:\n",
    "+ Our **input layer** represents input into the network. The size of the input layer is the size of the vocabulary of our corpus (+1).\n",
    "+ We then have an **embedding layer** immediately after the input layer, which will learn **word embeddings** for us (continuous representation of the discrete words in our vocabulary; see my explanatory blog post on embeddings [here](https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2). An embedding layer is just a fully-connected layer (with some regularisation and constraints), where the learned weight matrix functions as our word embeddings.\n",
    "+ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Keras` code, we would build this network like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=10, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(units=vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of this code block:\n",
    "+ Keras allows the sequential layer-by-layer building of neural network models using its `Sequential` API.\n",
    "+ The input layer is assumed, we don't need to explicitly build it.\n",
    "+ The first layer we add is our `Embedding` layer. The input dimensionality is our vocabulary size (the size of our input layer), and let's give this embedding layer a small size of 10 neurons. This means each word will get represented as a real-valued vector of length 10. We state the the length of inputs the network should expect is 2. \n",
    "+ Next, we add the workhorse of the network - our layer of `LSTM` neurons. Let's make the layer have 50 of these neurons (which is not a lot). We leave all other options to the default (activation functions, initialisation,etc.)\n",
    "+ Finally, as our output layer, we add a `Dense` fully-connected layer and softmax it. This means that the output of the network will be a vector of probabilities (summing to 1) spread across all the words of our vocabulary (see example below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine our model so far using Keras' `model.summary()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2, 10)             390       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 39)                1989      \n",
      "=================================================================\n",
      "Total params: 14,579\n",
      "Trainable params: 14,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summarises the number of parameters in our model and where they are.\n",
    "+ 390 parameters from $39*10$\n",
    "+ 12200 LSTM parameters from $4*(10*50 + 50*50 + 50)$\n",
    "+ 1989 parameters from $50*39 + 39$\n",
    "\n",
    "Now that we have defined the network, we need to do a `model.compile()` to signify that we have finished building the network and want to define how training should proceed. Specifically, we need to provide:\n",
    "+ Which loss function we want to use (i.e. what is the goal the model is optimising for as it trains, or what signal is it following in order to improve)\n",
    "+ Which optimiser we want to use to do our gradient updates (Adam, Adagrad, RMSProp, Nesterov momentum, etc.)\n",
    "+ Any metrics we want to calculate and output during training in order to keep track of progress. Let's keep track of accuracy, which is just the percentage of predictions that the model gets right. \n",
    "\n",
    "We can just use sensible defaults for now. Since our task is a multiclass classification task, a sensible loss metric to use is **categorical cross-entropy**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll avoid dumping equations on you and just say that:\n",
    "+ The model's categorical cross-entropy loss will be **low** when the network generally predicts the next words correctly. This means it tends to assign higher probability to the correct word.\n",
    "+ A training loss of zero means that the network always assigns a probability of 1 to the correct word and 0 to all other words - its predictions are perfect (in the training set).\n",
    "+ The model's categorical cross-entropy loss will be **high** when the network generally doesn't predict the next words well. This means it tends to incorrectly assign high probabilities to incorrect words.\n",
    "\n",
    "During the training process, the model optimises its internal parameters such that training loss is minimised (for an explanation of how this happens, read about backpropagation and gradient descent [here]())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Training the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the network is compiled, we can begin training it for some time (for some number of **epochs** - which is the number of times the network sees your training data). \n",
    "\n",
    "Hopefully, as model training proceeds, we will see that the training loss steadily decreases and the accuracy increases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13ea22b50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X, y, epochs=50, verbose=2)\n",
    "model.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the model trained! Training is very fast because our dataset is tiny and the network is small. The accuracy doesn't look that bad either (though of course the model is likely to be **overfitted**; see later section). \n",
    "\n",
    "There's a few different things you can do with a trained Keras sequence model. You can see all the options by typing `model.` followed by a `tab` in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33057487,  0.18980494, -0.364027  ,  0.30919576, -0.1292659 ,\n",
       "       -0.27887756,  0.38433874,  0.20016178,  0.19976257, -0.31896615],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.embeddings.Embedding at 0x13c6a7510>,\n",
       " <keras.layers.recurrent.LSTM at 0x103407450>,\n",
       " <keras.layers.core.Dense at 0x103448550>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Using the trained model to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the most interesting thing to do now is use the trained model to make new predictions. For this, we can use the `model.predict_classes()` method. \n",
    "\n",
    "We hope that the model will predict the next word given a seed sequence well, i.e. that it learned about word structure from our poem corpus. For instance, given the seed sequence \"shall be\", we hope the model predicts the correct, observed next words like \"king\", \"broken\", and \"blade\".\n",
    "\n",
    "However, we can't just run `model.predict_classes()` on raw text data like \"shall be\", since the text data has to first be tokenised, assigned to an integer index, and reshaped into the correct array dimensions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded seed sequence: [3, 5]\n",
      "Formatted encoded seed sequence: [[3 5]]\n"
     ]
    }
   ],
   "source": [
    "seed_sequence = 'shall be'\n",
    "seed_sequence_encoded = tokeniser.texts_to_sequences([seed_sequence])[0]\n",
    "print('Encoded seed sequence: %s' % seed_sequence_encoded)\n",
    "seed_sequence_encoded = np.array(seed_sequence_encoded).reshape(-1,2)\n",
    "print('Formatted encoded seed sequence: %s' % seed_sequence_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the trained model to make a prediction for the next word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the next word index: [38]\n",
      "This index corresponds to word: ['king']\n"
     ]
    }
   ],
   "source": [
    "prediction_index = model.predict_classes(seed_sequence_encoded)\n",
    "print('Prediction for the next word index: %s' % prediction_index)\n",
    "print('This index corresponds to word: %s' % tokeniser.sequences_to_texts([prediction_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that looks like a decent prediction for the next word!\n",
    "\n",
    "Rather than just have the 1 best prediction, it would be interesting to see the probabilities assigned to each possible next word. With a bit of manoeuvring we can get these scores out: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>probability</th>\n",
       "      <th>rounded_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>king</td>\n",
       "      <td>0.202408</td>\n",
       "      <td>0.20241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>woken</td>\n",
       "      <td>0.188621</td>\n",
       "      <td>0.18862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>a</td>\n",
       "      <td>0.157626</td>\n",
       "      <td>0.15763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>blade</td>\n",
       "      <td>0.147121</td>\n",
       "      <td>0.14712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>that</td>\n",
       "      <td>0.108378</td>\n",
       "      <td>0.10838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>be</td>\n",
       "      <td>0.021870</td>\n",
       "      <td>0.02187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>from</td>\n",
       "      <td>0.021261</td>\n",
       "      <td>0.02126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>fire</td>\n",
       "      <td>0.017142</td>\n",
       "      <td>0.01714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>frost</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.01553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>renewed</td>\n",
       "      <td>0.015003</td>\n",
       "      <td>0.01500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     word  probability  rounded_probability\n",
       "38     38     king     0.202408              0.20241\n",
       "28     28    woken     0.188621              0.18862\n",
       "11     11        a     0.157626              0.15763\n",
       "33     33    blade     0.147121              0.14712\n",
       "4       4     that     0.108378              0.10838\n",
       "5       5       be     0.021870              0.02187\n",
       "10     10     from     0.021261              0.02126\n",
       "27     27     fire     0.017142              0.01714\n",
       "25     25    frost     0.015533              0.01553\n",
       "32     32  renewed     0.015003              0.01500"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class_indices = list(range(0, vocabulary_size+1))\n",
    "\n",
    "df = pd.DataFrame(list(zip(class_indices, \n",
    "                      [tokeniser.sequences_to_texts([[index]])[0] for index in class_indices],\n",
    "                       model.predict(seed_sequence_encoded)[0],\n",
    "                       np.round(model.predict(seed_sequence_encoded)[0],5))),\n",
    "                  columns=['index', 'word', 'probability', 'rounded_probability'])\n",
    "\n",
    "df.sort_values('probability', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, it looks like the network does indeed assign the highest probabilities to the 3 words that actually occur in the corpus! It's fun to see that such a small network can produce sensible results on such a small dataset. \n",
    "\n",
    "Let's try another example with a different seed sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded seed sequence: [8, 2]\n",
      "Formatted encoded seed sequence: [[8 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>probability</th>\n",
       "      <th>rounded_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>not</td>\n",
       "      <td>0.193435</td>\n",
       "      <td>0.19344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>glitter</td>\n",
       "      <td>0.191043</td>\n",
       "      <td>0.19104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>wither</td>\n",
       "      <td>0.176577</td>\n",
       "      <td>0.17658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>are</td>\n",
       "      <td>0.111534</td>\n",
       "      <td>0.11153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>all</td>\n",
       "      <td>0.103988</td>\n",
       "      <td>0.10399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>lost</td>\n",
       "      <td>0.086964</td>\n",
       "      <td>0.08696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>reached</td>\n",
       "      <td>0.032819</td>\n",
       "      <td>0.03282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>by</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>0.02184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>those</td>\n",
       "      <td>0.019182</td>\n",
       "      <td>0.01918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>deep</td>\n",
       "      <td>0.014115</td>\n",
       "      <td>0.01412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     word  probability  rounded_probability\n",
       "2       2      not     0.193435              0.19344\n",
       "13     13  glitter     0.191043              0.19104\n",
       "20     20   wither     0.176577              0.17658\n",
       "9       9      are     0.111534              0.11153\n",
       "6       6      all     0.103988              0.10399\n",
       "17     17     lost     0.086964              0.08696\n",
       "23     23  reached     0.032819              0.03282\n",
       "24     24       by     0.021843              0.02184\n",
       "14     14    those     0.019182              0.01918\n",
       "21     21     deep     0.014115              0.01412"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_sequence = 'does not'\n",
    "seed_sequence_encoded = tokeniser.texts_to_sequences([seed_sequence])[0]\n",
    "print('Encoded seed sequence: %s' % seed_sequence_encoded)\n",
    "seed_sequence_encoded = np.array(seed_sequence_encoded).reshape(-1,2)\n",
    "print('Formatted encoded seed sequence: %s' % seed_sequence_encoded)\n",
    "df = pd.DataFrame(list(zip(class_indices, \n",
    "                      [tokeniser.sequences_to_texts([[index]])[0] for index in class_indices],\n",
    "                       model.predict(seed_sequence_encoded)[0],\n",
    "                       np.round(model.predict(seed_sequence_encoded)[0],5))),\n",
    "                  columns=['index', 'word', 'probability', 'rounded_probability'])\n",
    "df.sort_values('probability', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that also looks correct.\n",
    "\n",
    "Rather than predicting just the next 1 word, would be nice to just let the network write continuous text for us, given some seed sequence starting point. Let's package up the above code into a function that lets us do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_sequence(seed_sequence,\n",
    "                        length_to_write,\n",
    "                        model, \n",
    "                        tokeniser, \n",
    "                        input_length,\n",
    "                        verbose=True):\n",
    "    \"\"\"\n",
    "    Generates text using a trained language\n",
    "    model and seed sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    print('Using seed sequence: \"%s\"' % seed_sequence)\n",
    "    sequence = seed_sequence\n",
    "    \n",
    "    for i in range(length_to_write):\n",
    "        \n",
    "        # tokenise and encode the seed sequence\n",
    "        encoded_sequence = tokeniser.texts_to_sequences([sequence])[0]\n",
    "        assert len(encoded_sequence)>=input_length, \\\n",
    "            'ERROR: seed sequence must be at least %s words.' % input_length\n",
    "        encoded_sequence = encoded_sequence[-input_length:]\n",
    "        encoded_sequence = np.array(encoded_sequence).reshape(-1,input_length)\n",
    "\n",
    "        # predict the next word index and corresponding word\n",
    "        prediction_index = model.predict_classes(encoded_sequence)\n",
    "        prediction = tokeniser.sequences_to_texts([prediction_index])\n",
    "        \n",
    "        if verbose:\n",
    "            print('Sequence so far: %s' % sequence)\n",
    "            print('Seed sequence encoded: %s' % encoded_sequence)\n",
    "            print('Most likely next word is {0} (index {1})'.format(prediction, prediction_index[0]))\n",
    "\n",
    "        sequence += ' ' + prediction[0]\n",
    "    \n",
    "    print('Output:\\n' + sequence)\n",
    "    \n",
    "#     return sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"all that\"\n",
      "Sequence so far: all that\n",
      "Seed sequence encoded: [[6 4]]\n",
      "Most likely next word is ['is'] (index 7)\n",
      "Sequence so far: all that is\n",
      "Seed sequence encoded: [[4 7]]\n",
      "Most likely next word is ['gold'] (index 12)\n",
      "Sequence so far: all that is gold\n",
      "Seed sequence encoded: [[ 7 12]]\n",
      "Most likely next word is ['does'] (index 8)\n",
      "Sequence so far: all that is gold does\n",
      "Seed sequence encoded: [[12  8]]\n",
      "Most likely next word is ['not'] (index 2)\n",
      "Sequence so far: all that is gold does not\n",
      "Seed sequence encoded: [[8 2]]\n",
      "Most likely next word is ['not'] (index 2)\n",
      "Output: all that is gold does not not\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'all that is gold does not not'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_text_sequence(\"all that\", 5,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, let's write some more text, but let's turn off the verbosity of the function so we just get the final result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"the light\"\n",
      "Output: the light from the the that that\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the light from the the that that'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_text_sequence(\"the light\", 5,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's kind of artsy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, writing a longer passage this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All that is gold does not glitter Not all those who wander are lost; The old that is strong does not wither, Deep roots are not reached by the frost. From the ashes, a fire shall be woken, A light from the shadows shall spring; Renewed shall be blade that was broken, The crownless again shall be king'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"ashes are\"\n",
      "Output: ashes are does reached not the lost from the the that that\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ashes are does reached not the lost from the the that that'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_text_sequence(\"ashes are\", 10,\n",
    "                    model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tiny model only knows the few words in the poem so this is a bit gibberish :) But it's still interesting to see.\n",
    "\n",
    "This is pretty much all there is to a basic language model. Now, let's tackle a real corpus (Game of Thrones) and build a bigger, more powerful model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Building a language model for Game of Thrones text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technical approach we'll take to building a GoT language model is pretty similar, with the major difference being the dataset. We are going to need access to a lot of GoT text - preferably, both the books and the subtitles from the HBO show. \n",
    "\n",
    "### i. Identifying some datasets\n",
    "\n",
    "Interestingly, there seems to already be a rich ecosystem of technical work surrounding GoT content. \n",
    "\n",
    "Check out projects like:\n",
    "+ The [Network of Thrones](https://networkofthrones.wordpress.com/) blog for network analyses of characters (e.g. which character is the most 'central' to the story?)\n",
    "+ An [API of Ice and Fire](https://anapioficeandfire.com) for grabbing various structured data about the universe\n",
    "+ And [this Reddit post](https://www.reddit.com/r/datasets/comments/769nhw/game_of_thrones_datasets_xpost_from_rfreefolk/) for a list of various datasets compiled about GoT.\n",
    "\n",
    "Maybe it's just me, but even despite these resources, I still couldn't actually find the raw text from the books and TV show. \n",
    "\n",
    "I did eventually come across 2 Kaggle datasets that contained exactly what I wanted:\n",
    "1. [Plain text files of all the books](https://www.kaggle.com/muhammedfathi/game-of-thrones-book-files/download) \n",
    "2. [Subtitle data for the episodes](https://filmora.wondershare.com/video-editing-tips/game-of-thrones-subtitles.html)\n",
    "    \n",
    "A bit of initial manual + regex clean up later, and you get the files included in this repo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Grabbing all text data from the Game of Thrones books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've got a few books in our current directory in .txt format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these .txt files in the current directory:\n",
      "Book_1_A_Game_of_Thrones.txt\n",
      "Book_2_A_Clash_of_Kings.txt\n",
      "Book_3_A_Storm_of_Swords.txt\n",
      "Book_4_A_Feast_for_Crows.txt\n",
      "Book_5_A_Dance_with_Dragons.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "book_txt_files = sorted(glob.glob('*.txt'))\n",
    "print('Found these .txt files in the current directory:', *book_txt_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a function to extract all of the text in these files, glue it together, and flatten the resulting list of lists into a single mega GoT list of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iteration_utilities import flatten\n",
    "\n",
    "def grab_book_data(txt_files):\n",
    "    \"\"\"\n",
    "    Grabb text data from a set of text files.\n",
    "    \"\"\"\n",
    "\n",
    "    # keep all text segments in this list\n",
    "    all_text_segments = []   \n",
    "    \n",
    "    # iterate over each book file\n",
    "    for txt_file in txt_files:\n",
    "    \n",
    "        print('Extracting text from file \"%s\"...' % txt_file)\n",
    "        # open file\n",
    "        with open(txt_file, 'r') as file:\n",
    "            data = file.read()\n",
    "            print('Found {0} lines of text in this book.'.format(len(data.split('\\n'))))\n",
    "            print('First few lines:\\n %s\\n' % ' '.join(data.split('\\n')[0:5]))  \n",
    "            all_text_segments.append(data)\n",
    "            \n",
    "    return ''.join(list(flatten(all_text_segments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use it to put all the book text data in one place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file \"Book_1_A_Game_of_Thrones.txt\"...\n",
      "Found 14002 lines of text in this book.\n",
      "First few lines:\n",
      " A GAME OF THRONES  PROLOGUE  “We should start back,” Gared urged as the woods began to grow dark around them.\n",
      "\n",
      "Extracting text from file \"Book_2_A_Clash_of_Kings.txt\"...\n",
      "Found 15765 lines of text in this book.\n",
      "First few lines:\n",
      " A CLASH OF KINGS  PROLOGUE  The comet’s tail spread across the dawn, a red slash that bled above the crags of Dragonstone like a wound in the pink and purple sky.\n",
      "\n",
      "Extracting text from file \"Book_3_A_Storm_of_Swords.txt\"...\n",
      "Found 19641 lines of text in this book.\n",
      "First few lines:\n",
      " A STORM OF SWORDS  PROLOGUE  The day was grey and bitter cold, and the dogs would not take the scent.\n",
      "\n",
      "Extracting text from file \"Book_4_A_Feast_for_Crows.txt\"...\n",
      "Found 16225 lines of text in this book.\n",
      "First few lines:\n",
      " A FEAST FOR CROWS  PROLOGUE  Dragons,” said Mollander. He snatched a withered apple off the ground and tossed it hand to hand.\n",
      "\n",
      "Extracting text from file \"Book_5_A_Dance_with_Dragons.txt\"...\n",
      "Found 18889 lines of text in this book.\n",
      "First few lines:\n",
      " A DANCE WITH DRAGONS  PROLOGUE  The night was rank with the smell of man.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_data = grab_book_data(book_txt_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly summarise the amount of data we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in this corpus: 84518\n",
      "The number of words in this corpus: 1724951\n"
     ]
    }
   ],
   "source": [
    "# count lines and words\n",
    "print('The number of lines in this corpus: {0}\\n'\n",
    "      'The number of words in this corpus: {1}'.format(len(book_data.split('\\n')),\n",
    "                                                       len(book_data.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Grabbing all text data from the Game of Thrones show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subtitle data is a bit more complicated to grab because it's in JSON file format, and also frankly the text is a bit messy - there's markup tags, music note symbols, and various other odd non-textual things. \n",
    "\n",
    "We have the following `.json` subtitle files in our current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found these .json files in the current directory:\n",
      "Season_1_Subtitles.json\n",
      "Season_2_Subtitles.json\n",
      "Season_3_Subtitles.json\n",
      "Season_4_Subtitles.json\n",
      "Season_5_Subtitles.json\n",
      "Season_6_Subtitles.json\n",
      "Season_7_Subtitles.json\n"
     ]
    }
   ],
   "source": [
    "subtitle_json_files = sorted(glob.glob(\"*.json\"))\n",
    "print('Found these .json files in the current directory:', *subtitle_json_files, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to write a function to get the data out. The function below will:\n",
    "+ **Iterate** over a given list of json subtitle files, **open** each file and **parse** the json\n",
    "+ **Sort** the subtitles by index. At the moment, the indices are sorted as strings (so, e.g. '1' is followed by '11') so we need to convert the indices to integers and sort them numerically. This is important to get right because otherwise the subtitles are jumbled out of order! \n",
    "+ And finally we **extract** the subtitle text and **append** to a master list (which we reformat by flattening) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def grab_subtitle_data(subtitle_json_files, verbose=True):\n",
    "    \"\"\"\n",
    "    Grabbing GoT subtitle data from json files.\n",
    "    \"\"\"\n",
    "\n",
    "    # keep all text segments in this list\n",
    "    all_text_segments = []\n",
    "\n",
    "    # iterate over each subtitles file\n",
    "    for season, subtitles_file in enumerate(subtitle_json_files):\n",
    "\n",
    "        # open subtitle file\n",
    "        with open(subtitles_file, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # iterate over episodes in the season\n",
    "        for episode in data.keys():\n",
    "            episode_data = {int(key):value for key,value in data[episode].items()}\n",
    "            episode_data = sorted(episode_data.items()) # deal with sorting by line (as integer) s\n",
    "            episode_text_segments = list(dict(episode_data).values())\n",
    "            print('Found {0} text segments in Season {1} '\n",
    "                  'Episode \"{2}\".'.format(len(episode_text_segments), \n",
    "                                          season, \n",
    "                                          episode.split('.')[0]))\n",
    "            if verbose:\n",
    "                print('First few segments:\\n%s' % '\\n'.join(episode_text_segments[0:5]))            \n",
    "            all_text_segments.append(episode_text_segments)\n",
    "            \n",
    "    return list(flatten(all_text_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 559 text segments in Season 0 Episode \"Game Of Thrones S01E01 Winter Is Coming\".\n",
      "Found 571 text segments in Season 0 Episode \"Game Of Thrones S01E02 The Kingsroad\".\n",
      "Found 740 text segments in Season 0 Episode \"Game Of Thrones S01E03 Lord Snow\".\n",
      "Found 754 text segments in Season 0 Episode \"Game Of Thrones S01E04 Cripples, Bastards, And Broken Things\".\n",
      "Found 741 text segments in Season 0 Episode \"Game Of Thrones S01E05 The Wolf And The Lion\".\n",
      "Found 583 text segments in Season 0 Episode \"Game Of Thrones S01E06 A Golden Crown\".\n",
      "Found 775 text segments in Season 0 Episode \"Game Of Thrones S01E07 You Win Or You Die\".\n",
      "Found 666 text segments in Season 0 Episode \"Game Of Thrones S01E08 The Pointy End\".\n",
      "Found 679 text segments in Season 0 Episode \"Game Of Thrones S01E09 Baelor\".\n",
      "Found 590 text segments in Season 0 Episode \"Game Of Thrones S01E10 Fire And Blood\".\n",
      "Found 700 text segments in Season 1 Episode \"Game Of Thrones S02E01 The North Remembers\".\n",
      "Found 755 text segments in Season 1 Episode \"Game Of Thrones S02E02 The Night Lands\".\n",
      "Found 654 text segments in Season 1 Episode \"Game Of Thrones S02E03 What Is Dead May Never Die\".\n",
      "Found 619 text segments in Season 1 Episode \"Game Of Thrones S02E04 Garden Of Bones\".\n",
      "Found 781 text segments in Season 1 Episode \"Game Of Thrones S02E05 The Ghost Of Harrenhal\".\n",
      "Found 730 text segments in Season 1 Episode \"Game Of Thrones S02E06 The Old Gods And The New\".\n",
      "Found 762 text segments in Season 1 Episode \"Game Of Thrones S02E07 A Man Without Honor\".\n",
      "Found 775 text segments in Season 1 Episode \"Game Of Thrones S02E08 The Prince Of Winterfell\".\n",
      "Found 640 text segments in Season 1 Episode \"Game Of Thrones S02E09 Blackwater\".\n",
      "Found 641 text segments in Season 1 Episode \"Game Of Thrones S02E10 Valar Morghulis\".\n",
      "Found 637 text segments in Season 2 Episode \"Game Of Thrones S03E01 Valar Dohaeris\".\n",
      "Found 778 text segments in Season 2 Episode \"Game Of Thrones S03E02 Dark Wings, Dark Words\".\n",
      "Found 661 text segments in Season 2 Episode \"Game Of Thrones S03E03 Walk Of Punishment\".\n",
      "Found 703 text segments in Season 2 Episode \"Game Of Thrones S03E04 And Now His Watch Is Ended\".\n",
      "Found 821 text segments in Season 2 Episode \"Game Of Thrones S03E05 Kissed By Fire\".\n",
      "Found 652 text segments in Season 2 Episode \"Game Of Thrones S03E06 The Climb\".\n",
      "Found 714 text segments in Season 2 Episode \"Game Of Thrones S03E07 The Bear And The Maiden Fair\".\n",
      "Found 576 text segments in Season 2 Episode \"Game Of Thrones S03E08 Second Sons\".\n",
      "Found 524 text segments in Season 2 Episode \"Game Of Thrones S03E09 The Rains Of Castamere\".\n",
      "Found 785 text segments in Season 2 Episode \"Game Of Thrones S03E10 Mhysa\".\n",
      "Found 756 text segments in Season 3 Episode \"Game Of Thrones S04E01 Two Swords\".\n",
      "Found 636 text segments in Season 3 Episode \"Game Of Thrones S04E02 The Lion And The Rose\".\n",
      "Found 753 text segments in Season 3 Episode \"Game Of Thrones S04E03 Breaker Of Chains\".\n",
      "Found 634 text segments in Season 3 Episode \"Game Of Thrones S04E04 Oathkeeper\".\n",
      "Found 664 text segments in Season 3 Episode \"Game Of Thrones S04E05 First Of His Name\".\n",
      "Found 622 text segments in Season 3 Episode \"Game Of Thrones S04E06 The Laws Of Gods And Men\".\n",
      "Found 695 text segments in Season 3 Episode \"Game Of Thrones S04E07 Mockingbird\".\n",
      "Found 662 text segments in Season 3 Episode \"Game Of Thrones S04E08 The Mountain And The Viper\".\n",
      "Found 451 text segments in Season 3 Episode \"Game Of Thrones S04E09 The Watchers On The Wall\".\n",
      "Found 613 text segments in Season 3 Episode \"Game Of Thrones S04E10 The Children\".\n",
      "Found 0 text segments in Season 3 Episode \"season4\".\n",
      "Found 631 text segments in Season 4 Episode \"Game Of Thrones S05E01 The Wars To Come\".\n",
      "Found 748 text segments in Season 4 Episode \"Game Of Thrones S05E02 The House Of Black And White\".\n",
      "Found 782 text segments in Season 4 Episode \"Game Of Thrones S05E03 High Sparrow\".\n",
      "Found 601 text segments in Season 4 Episode \"Game Of Thrones S05E04 Sons Of The Harpy\".\n",
      "Found 647 text segments in Season 4 Episode \"Game Of Thrones S05E05 Kill The Boy\".\n",
      "Found 628 text segments in Season 4 Episode \"Game Of Thrones S05E06 Unbowed, Unbent, Unbroken\".\n",
      "Found 700 text segments in Season 4 Episode \"Game Of Thrones S05E07 The Gift\".\n",
      "Found 658 text segments in Season 4 Episode \"Game Of Thrones S05E08 Hardhome\".\n",
      "Found 481 text segments in Season 4 Episode \"Game Of Thrones S05E09 The Dance Of Dragons\".\n",
      "Found 563 text segments in Season 4 Episode \"Game Of Thrones S05E10 Mother's Mercy\".\n",
      "Found 385 text segments in Season 5 Episode \"Game Of Thrones S06E01 The Red Woman\".\n",
      "Found 480 text segments in Season 5 Episode \"Game Of Thrones S06E02 Home\".\n",
      "Found 598 text segments in Season 5 Episode \"Game Of Thrones S06E03 Oathbreaker\".\n",
      "Found 640 text segments in Season 5 Episode \"Game Of Thrones S06E04 Book of the Stranger\".\n",
      "Found 644 text segments in Season 5 Episode \"Game Of Thrones S06E05 The Door\".\n",
      "Found 605 text segments in Season 5 Episode \"Game Of Thrones S06E06 Blood of My Blood\".\n",
      "Found 615 text segments in Season 5 Episode \"Game Of Thrones S06E07 The Broken Man\".\n",
      "Found 661 text segments in Season 5 Episode \"Game Of Thrones S06E08 No One\".\n",
      "Found 446 text segments in Season 5 Episode \"Game Of Thrones S06E09 Battle of the Bastards\".\n",
      "Found 605 text segments in Season 5 Episode \"Game Of Thrones S06E10 The Winds of Winter\".\n",
      "Found 753 text segments in Season 6 Episode \"Game Of Thrones S07E01 Dragonstone\".\n",
      "Found 812 text segments in Season 6 Episode \"Game Of Thrones S07E02 Stormborn\".\n",
      "Found 983 text segments in Season 6 Episode \"Game Of Thrones S07E03 The Queen's Justice\".\n",
      "Found 608 text segments in Season 6 Episode \"Game Of Thrones S07E04 The Spoils Of War\".\n",
      "Found 764 text segments in Season 6 Episode \"Game Of Thrones S07E05 Eastwatch\".\n",
      "Found 796 text segments in Season 6 Episode \"Game Of Thrones S07E06 Beyond The Wall\".\n",
      "Found 958 text segments in Season 6 Episode \"Game Of Thrones S07E07 The Dragon And The Wolf\".\n"
     ]
    }
   ],
   "source": [
    "subtitle_data = grab_subtitle_data(subtitle_json_files, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final array of subtitle data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Easy, boy.',\n",
       " \"What do you expect? They're savages.\",\n",
       " 'One lot steals a goat from another lot,',\n",
       " \"before you know it they're ripping each other to pieces.\",\n",
       " \"I've never seen wildlings do a thing like this.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitle_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can summarise the dataset size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of text segments in this corpus: 44844\n",
      "The number of words in this corpus: 244447\n"
     ]
    }
   ],
   "source": [
    "# count lines and words\n",
    "all_subtitle_text = '\\n'.join(subtitle_data)\n",
    "print('The number of text segments in this corpus: {0}\\n'\n",
    "      'The number of words in this corpus: {1}'.format(len(all_subtitle_text.split('\\n')),\n",
    "                                                       len(all_subtitle_text.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Combining the book and subtitle datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put the book and subtitle data together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_data = book_data+all_subtitle_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And report on the size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in the final corpus: 129361\n",
      "The number of words in the final corpus: 1969397\n"
     ]
    }
   ],
   "source": [
    "print('The number of lines in the final corpus: {0}\\n'\n",
    "      'The number of words in the final corpus: {1}'.format(len(got_data.split('\\n')),\n",
    "                                                            len(got_data.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 2 million words to play with, which should help our language model tremendously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process to make the sequence datasets is the same as before. The only difference is that we'll use longer sequences as our input (`window_size` is now 6), so we're taking into account more text before making our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise the data\n",
    "tokeniser = Tokenizer(lower=True, split=' ', char_level=False)\n",
    "tokeniser.fit_on_texts([got_data])\n",
    "vocabulary_size = len(tokeniser.word_index)+1\n",
    "print('The vocabulary size for this corpus is: %s' % vocabulary_size)\n",
    "\n",
    "# encode the corpus using the fitted tokeniser\n",
    "encoded_corpus = tokeniser.texts_to_sequences([got_data])[0]\n",
    "\n",
    "# generate sequences\n",
    "sequences = []\n",
    "window_size = 6\n",
    "for i in range(0, len(encoded_corpus)):\n",
    "    sequences.append(encoded_corpus[i:i+window_size])\n",
    "\n",
    "# pad the sequences at the end so each sequence is the same length\n",
    "max_sequence_length = np.max([len(sequence) for sequence in sequences])\n",
    "sequences = pad_sequences(sequences, \n",
    "                                maxlen=max_sequence_length, \n",
    "                                padding='pre')\n",
    "\n",
    "# separate sequences into input arrays X \n",
    "# and the output label vector y\n",
    "X = np.array([seq[0:window_size-1] for seq in sequences])\n",
    "y = np.array([seq[window_size-1] for seq in sequences])\n",
    "y = to_categorical(y, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-b543e2fdecf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoT_X_features.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoT_y_labels.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 542\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.save('GoT_X_features.npz', X)\n",
    "np.save('GoT_y_labels.npz', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, our features look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    5,   972,     6,  3796, 12141],\n",
       "       [  972,     6,  3796, 12141,   322],\n",
       "       [    6,  3796, 12141,   322,   122],\n",
       "       [ 3796, 12141,   322,   122,  1131],\n",
       "       [12141,   322,   122,  1131,    62]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our labels look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful extra step: we should **split the dataset into a train and test set**. The main reason for this is that it will help us get a better estimate of the model's true \"in the wild\" performance, since we can evaluate its performance on data that *wasn't* used in training. \n",
    "\n",
    "Evaluating a model on data that was used for training is cheating, since it's already seen that data before, and hence will do unrealistically well when making predictions on it because it has **overfit**.\n",
    "\n",
    "We will also shuffle the entries, since otherwise our dataset first contains Book 1, then Book 2, ..., Book 5 then finally the subtitle data, whereas we want the model to learn from each source simultaneously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_X = X[0:1000]\n",
    "small_y = y[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(small_X, small_y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Setting up the language model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's build a slightly larger network:\n",
    "\n",
    "![Larger RNN language model](big_network.png)\n",
    "\n",
    "The main differences here are:\n",
    "+ Our word embeddings are bigger (100 rather than 50 dimensions)\n",
    "+ We have 2 LSTM layers instead of 1. This should allow the model to learn more complex representations of the text.\n",
    "+ We have added a dense (fully-connected) layer after the LSTM layers for some additional processing capacity (perhaps allowing for higher-level conceptual representations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Keras` code, we would build the network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, 50, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very similar code to before, but we have reason to think that this network will be much more complex and nuanced than the previous one:\n",
    "+ The dataset we are using is much larger and richer than the toy dataset\n",
    "+ The network we are training is larger and deeper, and should have more expressive power\n",
    "\n",
    "We can summarise the **model structure and parameters**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5, 50)             1517500   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30350)             3065350   \n",
      "=================================================================\n",
      "Total params: 4,733,750\n",
      "Trainable params: 4,733,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compile the finished model and specify some **training settings**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vii. Training the language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can start the training run by passing the training data to the model. This would take a reasonably long time to train - it would be helpful to have access to a **GPU** to run this on (e.g. via Google Colab, AWS/GCP, your own GPU) to make use of computation **parallelisation** and drastically reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "900000/900000 [==============================] - 1102s 1ms/step - loss: 6.2328 - accuracy: 0.0989\n",
      "Epoch 2/2\n",
      "900000/900000 [==============================] - 1105s 1ms/step - loss: 5.5791 - accuracy: 0.1346\n"
     ]
    }
   ],
   "source": [
    "# # UNCOMMENT AND RUN THIS CELL TO TRAIN MODEL YOURSELF\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=2, verbose=1)\n",
    "model.save(\"trained_GoT_language_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, to save time, I will just **load a model** that I already trained. \n",
    "\n",
    "For reference, this was just trained overnight on my laptop so there's no special supercomputer involved. The model was still improving quite rapidly at that point, so we would see even better performance if the model were given enough time to reach **convergence** (\"finish\" learning, or at least hit serious diminishing returns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model('trained_GoT_language_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viii. Exploring our Game of Thrones language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training log, I saw that the model's training loss after epoch 50 was around 4.2, and the accuracy around 0.23. \n",
    "\n",
    "We can summarise the model's performance on the training and test set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([138,   5,  42,  34,   1,  56,   2,   6,   3,  34,  34,   4,  56,\n",
       "        11,   3,   8,   1,   1, 129,   6,  53,   2,   1,  11,  53,   8,\n",
       "         3,   1,   2,   4,  53,   1,   1, 158,   1,   1,   1,   2,   4,\n",
       "         6, 149,   6, 132,  40,   8,  53,   1,   2,   2,   2,   5,   5,\n",
       "         6,   2,   6,   2,  56,   5,   1,  56,   6,  53,   2,  46,   1,\n",
       "        11,   4,   5,   1,   3,  29,   5,   4,  29,   4,  26,   2,   1,\n",
       "        69,   1,  11,   1,  73,   2,   5,  16,   4,   1,  56,   5,   4,\n",
       "         6,   2,  17,   4,   4,   7,   1,   2,   1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([138,   5,  42,  34,   1,  56,   2,   6,   3,  34,  34,   4,  56,\n",
       "        11,   3,   8,   1,   1, 129,   6,  53,   2,   1,  11,  53,   8,\n",
       "         3,   1,   2,   4,  53,   1,   1, 158,   1,   1,   1,   2,   4,\n",
       "         6, 149,   6, 132,  40,   8,  53,   1,   2,   2,   2,   5,   5,\n",
       "         6,   2,   6,   2,  56,   5,   1,  56,   6,  53,   2,  46,   1,\n",
       "        11,   4,   5,   1,   3,  29,   5,   4,  29,   4,  26,   2,   1,\n",
       "        69,   1,  11,   1,  73,   2,   5,  16,   4,   1,  56,   5,   4,\n",
       "         6,   2,  17,   4,   4,   7,   1,   2,   1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(loaded_model.predict_classes(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = loaded_model.predict_classes(X_train)\n",
    "test_predictions = loaded_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall training accuracy: 0.16666666666666666\n",
      "Overall test accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "print('Overall training accuracy: {0}\\n'\n",
    "      'Overall test accuracy: {1}'.format(accuracy_score(np.argmax(y_train, axis=1), train_predictions),\n",
    "                                          accuracy_score(np.argmax(y_test, axis=1), test_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this performance mean in practical terms? We can examine some of the predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seed_sequences = tokeniser.sequences_to_texts(X_test[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_next_words = tokeniser.sequences_to_texts([np.argmax(y_test, axis=1)[0:50]])[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_index = loaded_model.predict_classes(X_test[0:50])\n",
    "prediction_vector = tokeniser.sequences_to_texts([prediction_index])\n",
    "predictions = prediction_vector[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bowels',\n",
       " 'sent',\n",
       " 'fire',\n",
       " 'royce',\n",
       " 'me',\n",
       " 'prepared',\n",
       " 'will',\n",
       " 'had',\n",
       " 'he',\n",
       " 'glanced',\n",
       " 'royce',\n",
       " '”',\n",
       " 'shared',\n",
       " 'must',\n",
       " 'all',\n",
       " 'this',\n",
       " 'nine',\n",
       " 'close',\n",
       " 'could',\n",
       " 'leave',\n",
       " 'man',\n",
       " 'maybe',\n",
       " 'fifty',\n",
       " 'wished',\n",
       " 'ride',\n",
       " 'they',\n",
       " 'and',\n",
       " 'will',\n",
       " 'gared',\n",
       " 'you',\n",
       " 'handsome',\n",
       " 'snow’s',\n",
       " 'no',\n",
       " 'mallisters’',\n",
       " 'a',\n",
       " '“did',\n",
       " 'boy',\n",
       " 'on',\n",
       " 'riding',\n",
       " 'years',\n",
       " 'will',\n",
       " 'is',\n",
       " 'tit',\n",
       " 'put',\n",
       " 'you',\n",
       " 'few',\n",
       " 'bored',\n",
       " 'black',\n",
       " '“we',\n",
       " 'he']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_next_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seed Sequence</th>\n",
       "      <th>Actual Next Word</th>\n",
       "      <th>Predicted Next Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>come rushing back and his</td>\n",
       "      <td>bowels</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>first time he had been</td>\n",
       "      <td>sent</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>still make it out no</td>\n",
       "      <td>fire</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>is falling ” ser waymar</td>\n",
       "      <td>royce</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>dead that’s proof enough for</td>\n",
       "      <td>me</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>could say he had not</td>\n",
       "      <td>prepared</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>that under the wounded pride</td>\n",
       "      <td>will</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>southron called the haunted forest</td>\n",
       "      <td>had</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>half bored half distracted way</td>\n",
       "      <td>he</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>falling ” ser waymar royce</td>\n",
       "      <td>glanced</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>frighten you ” ser waymar</td>\n",
       "      <td>royce</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>we ” “will saw them</td>\n",
       "      <td>”</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>warrior ” they had all</td>\n",
       "      <td>shared</td>\n",
       "      <td>been</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>shivering atop his garron gared</td>\n",
       "      <td>must</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>trees rustle like living things</td>\n",
       "      <td>all</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>before us i don’t like</td>\n",
       "      <td>this</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>pointed out “eight days maybe</td>\n",
       "      <td>nine</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>nervous tension that came perilous</td>\n",
       "      <td>close</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>covered it now but i</td>\n",
       "      <td>could</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>saw will all the details</td>\n",
       "      <td>leave</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>something else in the older</td>\n",
       "      <td>man</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>gared pointed out “eight days</td>\n",
       "      <td>maybe</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>was an old man past</td>\n",
       "      <td>fifty</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>quarrel sooner or later he</td>\n",
       "      <td>wished</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>forest “we have a long</td>\n",
       "      <td>ride</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>with the dead ” “are</td>\n",
       "      <td>they</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>as we should track them</td>\n",
       "      <td>and</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>the woods as silent as</td>\n",
       "      <td>will</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>black hood of his cloak</td>\n",
       "      <td>gared</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>royce replied “never believe anything</td>\n",
       "      <td>you</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>many heirs he was a</td>\n",
       "      <td>handsome</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>a fortnight getting back and</td>\n",
       "      <td>snow’s</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>than half a year but</td>\n",
       "      <td>no</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>woods skinning one of the</td>\n",
       "      <td>mallisters’</td>\n",
       "      <td>wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>could see they put up</td>\n",
       "      <td>a</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>“well no ” will admitted</td>\n",
       "      <td>“did</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>the night’s watch man and</td>\n",
       "      <td>boy</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>farther from the wall hard</td>\n",
       "      <td>on</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>us no more there’s hard</td>\n",
       "      <td>riding</td>\n",
       "      <td>”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>cloak gared had spent forty</td>\n",
       "      <td>years</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>gared had felt it too</td>\n",
       "      <td>will</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>discover his talent “the camp</td>\n",
       "      <td>is</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>you hear at a woman’s</td>\n",
       "      <td>tit</td>\n",
       "      <td>brother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>children i could see they</td>\n",
       "      <td>put</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>” “do the dead frighten</td>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>weapons ” “some swords a</td>\n",
       "      <td>few</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>deepening twilight in that half</td>\n",
       "      <td>bored</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>leather boots black woolen pants</td>\n",
       "      <td>black</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>loud in the twilit forest</td>\n",
       "      <td>“we</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>fear will shared his unease</td>\n",
       "      <td>he</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Seed Sequence Actual Next Word Predicted Next Word\n",
       "0               come rushing back and his           bowels                 own\n",
       "1                  first time he had been             sent                   a\n",
       "2                    still make it out no             fire                 one\n",
       "3                 is falling ” ser waymar            royce                said\n",
       "4            dead that’s proof enough for               me                 the\n",
       "5                    could say he had not         prepared                been\n",
       "6            that under the wounded pride             will                 and\n",
       "7      southron called the haunted forest              had                  of\n",
       "8          half bored half distracted way               he                  to\n",
       "9              falling ” ser waymar royce          glanced                said\n",
       "10              frighten you ” ser waymar            royce                said\n",
       "11                    we ” “will saw them                ”                   ”\n",
       "12                 warrior ” they had all           shared                been\n",
       "13        shivering atop his garron gared             must                 was\n",
       "14        trees rustle like living things              all                  to\n",
       "15                 before us i don’t like             this                 you\n",
       "16          pointed out “eight days maybe             nine                 the\n",
       "17     nervous tension that came perilous            close                 the\n",
       "18                   covered it now but i            could                  am\n",
       "19               saw will all the details            leave                  of\n",
       "20            something else in the older              man                 man\n",
       "21          gared pointed out “eight days            maybe                 and\n",
       "22                    was an old man past            fifty                 the\n",
       "23             quarrel sooner or later he           wished                 was\n",
       "24                 forest “we have a long             ride                 man\n",
       "25                   with the dead ” “are             they                 you\n",
       "26                as we should track them              and                  to\n",
       "27                 the woods as silent as             will                 the\n",
       "28                black hood of his cloak            gared                 and\n",
       "29  royce replied “never believe anything              you                   ”\n",
       "30                    many heirs he was a         handsome                 man\n",
       "31           a fortnight getting back and           snow’s                 the\n",
       "32                   than half a year but               no                 the\n",
       "33              woods skinning one of the      mallisters’                wall\n",
       "34                  could see they put up                a                 the\n",
       "35               “well no ” will admitted             “did                 the\n",
       "36              the night’s watch man and              boy                 the\n",
       "37             farther from the wall hard               on                 and\n",
       "38                us no more there’s hard           riding                   ”\n",
       "39            cloak gared had spent forty            years                  of\n",
       "40                  gared had felt it too             will                much\n",
       "41          discover his talent “the camp               is                  of\n",
       "42                  you hear at a woman’s              tit             brother\n",
       "43              children i could see they              put                were\n",
       "44                ” “do the dead frighten              you                 you\n",
       "45               weapons ” “some swords a              few                 man\n",
       "46        deepening twilight in that half            bored                 the\n",
       "47       leather boots black woolen pants            black                 and\n",
       "48              loud in the twilit forest              “we                 and\n",
       "49            fear will shared his unease               he                 and"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(test_seed_sequences,\n",
    "                           actual_next_words,\n",
    "                           predictions)),\n",
    "                  columns=['Seed Sequence', 'Actual Next Word', 'Predicted Next Word'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like even where the model doesn't get the prediction correct, its prediction does at least seem plausible. \n",
    "\n",
    "With something as flexible as language, perhaps relatively low accuracies are not the end of the world. That said, there is tons that can be done to improve this model (see final section in this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ix. Grand Finale: Gather Round for a New Tale..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have the language model write some Game of Thrones text for us (since GRR Martin certainly isn't going to!). \n",
    "\n",
    "We can use the same function as before to continuously feed in a seed sequence to the model, generate one word, and then append the generated word to the seed sequence. In this way, the model uses its own previous output as input to itself in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed sequence: \"The start of the story\"\n",
      "Output:\n",
      "The start of the story ” he said and the king was a man and the king ” he said and the king was a man and the king ” he said and the king was a man and the king ” he said and the king was a man and the king ” he\n"
     ]
    }
   ],
   "source": [
    "write_text_sequence(\"The start of the story\", 50,\n",
    "                    loaded_model, tokeniser, \n",
    "                    max_sequence_length-1,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"trained_GoT_language_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the language model network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-67f89bf44b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary_size' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, 50, input_length=max_sequence_length-1))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 5, 50)             1517500   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 5, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30350)             3065350   \n",
      "=================================================================\n",
      "Total params: 4,733,750\n",
      "Trainable params: 4,733,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training the network. This one will take significantly longer to train because:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6ef3a0ba4d08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61512f76fa46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trained_GoT_language_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"trained_GoT_language_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Suggested Extensions\n",
    "\n",
    "Here are some suggestions for extending this work in order to build a more serious Game of Thrones language model:\n",
    "\n",
    "1. **Data**: Spend more time cleaning up the text corpus, there is definitely some weird stuff in there (e.g. I saw markup tags in the subtitle data)\n",
    "2. **Data**: Perhaps think about grabbing more data, maybe by scraping some of the fan Wikis.\n",
    "3. **Representation**: Use pre-learned word embeddings (e.g. FastText, GloVe, Word2Vec) and possibly update them during training\n",
    "4. **Representation**: Think about using sub-word tokenisation rather than word-based tokenisation\n",
    "8. **Modelling**: Probably the most important thing - train for longer, until convergence :) Ideally, monitor for overfitting using a validation set. \n",
    "5. **Modelling**: Look into using regularisation techniques (dropout, weight penalties) to improve model performance and generalisability\n",
    "6. **Modelling**: Experiment with different numbers of layers, sizes, activation functions, initialisation approaches, etc.\n",
    "7. **Modelling**: Optimise some of the hyperparameters in the model (learning rate, momentum, batch sizes)\n",
    "9. **Modelling**: Wildcard idea - forget RNNs for language modelling completely and jump on the Transformer hype train ([choo](https://paperswithcode.com/task/language-modelling) [choo!](https://arxiv.org/abs/1904.09408)). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
